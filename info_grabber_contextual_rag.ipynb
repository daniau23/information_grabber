{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace\n",
    "# from langchain.llms import HuggingFaceHub\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "# Prompts\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt,\\\n",
    "    PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import HumanMessage, SystemMessage,AIMessage\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector,\\\n",
    "    SemanticSimilarityExampleSelector\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "# Output parsing\n",
    "from langchain.output_parsers import PydanticOutputParser,\\\n",
    "    OutputFixingParser,CommaSeparatedListOutputParser,\\\n",
    "        RetryWithErrorOutputParser,\\\n",
    "        StructuredOutputParser,ResponseSchema\n",
    "# from pydantic import BaseModel,Field,field_validator\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import List,Dict\n",
    "# Scapers,Retrievers,Loaders,Splitters,Embeddings\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.chains import RetrievalQA,LLMChain,\\\n",
    "    ConversationalRetrievalChain, ConversationChain,\\\n",
    "        SimpleSequentialChain,RetrievalQAWithSourcesChain\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,\\\n",
    "    CharacterTextSplitter,NLTKTextSplitter,SpacyTextSplitter,\\\n",
    "    MarkdownTextSplitter,TokenTextSplitter,Language\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.document_loaders import PyPDFLoader, SeleniumURLLoader,\\\n",
    "    GoogleDriveLoader\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor,\\\n",
    "    LLMChainFilter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_cohere import CohereEmbeddings, \\\n",
    "    CohereRerank as langchain_cohere_reranker\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "# ConstitutionalAI\n",
    "from langchain.chains.constitutional_ai.base import ConstitutionalChain\n",
    "from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\n",
    "# Conversation, Chat & Memory\n",
    "from langchain.prompts.chat import ChatPromptTemplate,\\\n",
    "    SystemMessagePromptTemplate,\\\n",
    "        AIMessagePromptTemplate,HumanMessagePromptTemplate,\\\n",
    "        MessagesPlaceholder\n",
    "from langchain import ConversationChain\n",
    "from  langchain.memory import ConversationBufferMemory,\\\n",
    "    ConversationSummaryBufferMemory, ConversationSummaryMemory,\\\n",
    "    ConversationBufferWindowMemory\n",
    "# Agents and Tools\n",
    "from langchain.agents import  tool # this is a decorator\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentType,initialize_agent,\\\n",
    "    load_tools, Tool,\\\n",
    "        AgentExecutor,\\\n",
    "        create_json_agent,create_react_agent,create_structured_chat_agent\n",
    "from langchain_google_community import GoogleSearchAPIWrapper\n",
    "from langchain.tools import DuckDuckGoSearchRun,DuckDuckGoSearchResults\n",
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain_experimental.autonomous_agents import BabyAGI, AutoGPT\n",
    "from langchain.tools.file_management import WriteFileTool,ReadFileTool\n",
    "from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
    "import faiss\n",
    "# LlamaIndex\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import download_loader\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.core.service_context import ServiceContext\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine,\\\n",
    "    SubQuestionQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.core.indices import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool,ToolMetadata\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.evaluation import generate_question_context_pairs,\\\n",
    "    EmbeddingQAFinetuneDataset,RelevancyEvaluator,\\\n",
    "    FaithfulnessEvaluator,BatchEvalRunner,RetrieverEvaluator\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.instructor import InstructorEmbedding\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank as llama_index_cohere_reranker\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank,LLMRerank\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI as llama_hugging_face_api\n",
    "from llama_index.llms.text_generation_inference import TextGenerationInference\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.readers.github import GithubRepositoryReader,GithubClient\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings as langchain_embeddings_hugging_face_api\n",
    "# from llama_index.llms.langchain import LangChainLLM # causing conflicts\n",
    "# DeepLake\n",
    "import deeplake\n",
    "# Others\n",
    "from newspaper import Article\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "from elevenlabs.client import ElevenLabs\n",
    "from streamlit_chat import message\n",
    "from audio_recorder_streamlit import audio_recorder\n",
    "import streamlit as st\n",
    "import cohere\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import seaborn as sns\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import yt_dlp\n",
    "import whisper\n",
    "import textwrap\n",
    "import re\n",
    "import tiktoken\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Provide the filename as a string\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# generator_repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# generator_repo_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "generator_repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_new_tokens\": 5000, # Maximum tokens to generate\n",
    "    \"max_length\": 6000, # Maximum length of input + output\n",
    "    \"temperature\": 0.1, # Controls randomness of output\n",
    "    \"timeout\": 6000,\n",
    "    # \"task\":'conversational',\n",
    "}\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=generator_repo_id,\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN,\n",
    "    # you specify the task or not\n",
    "    # You can also specify the task in the model_kwargs or within here\n",
    "    # task = 'conversational',\n",
    "    **model_kwargs,\n",
    ")\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Contextual generator\n",
    "model_kwargs_contextual = {\n",
    "    \"max_new_tokens\": 5000, # Maximum tokens to generate\n",
    "    \"max_length\": 6000, # Maximum length of input + output\n",
    "    \"temperature\": 0.1, # Controls randomness of output\n",
    "    \"timeout\": 6000,\n",
    "    # \"task\":'conversational',\n",
    "}\n",
    "\n",
    "\n",
    "llm_contextual = HuggingFaceEndpoint(\n",
    "    repo_id=contextual_repo_id,\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN,\n",
    "    # you specify the task or not\n",
    "    # You can also specify the task in the model_kwargs or within here\n",
    "    # task = 'conversational',\n",
    "    **model_kwargs_contextual,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop = os.path.expanduser(\"~\\Desktop\")\n",
    "llmai = os.path.join(desktop,'llmai')\n",
    "llm = os.path.join(llmai,'llms')\n",
    "projects = os.path.join(llm,'my_llm_projects')\n",
    "information_bot = os.path.join(projects,'information_grabbing')\n",
    "resources = os.path.join(information_bot,\"resources\")\n",
    "books = os.path.join(resources,\"book1.pdf\")\n",
    "print(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(books)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generating the full document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = ['gggg', 'tyga']\n",
    "# \" \".join(g)\n",
    "# 'gggg tyga'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_doc = []\n",
    "for page in range(len(pages)):\n",
    "    full_doc.append(pages[page].page_content.strip())\n",
    "\n",
    "full_document = \" \".join(full_doc).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"resources/myfile.txt\", \"w\", encoding=\"utf-8\") as file1:\n",
    "#     file1.write(full_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of documents:\\n{len(pages)}\\n\")\n",
    "print(f\"The whole documents:\\n{pages}\\n\")\n",
    "print(f\"Taking a document from the book:\\n{pages[0]}\\n\")\n",
    "print(f\"Taking a page content from the book:\\n{pages[0].page_content}\\n\")\n",
    "print(f\"Taking a meta data from the book:\\n{pages[0].metadata}\\n\")\n",
    "print(f\"Meta data keys: {list(pages[0].metadata.keys())}\\n\")\n",
    "print(f\"Taking a source from meta data from the book:\\n{pages[0].metadata['source']}\\n\")\n",
    "print(f\"Taking a page from meta data from the book:\\n{pages[0].metadata['page']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=500)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of documents:\\n{len(docs)}\\n\")\n",
    "print(f\"The whole documents:\\n{docs}\\n\")\n",
    "print(f\"Taking a document from the book:\\n{docs[0]}\\n\")\n",
    "print(f\"Taking a page content from the book:\\n{docs[0].page_content}\\n\")\n",
    "print(f\"Taking a meta data from the book:\\n{docs[0].metadata}\\n\")\n",
    "print(f\"Meta data keys: {list(docs[0].metadata.keys())}\\n\")\n",
    "print(f\"Taking a source from meta data from the book:\\n{docs[0].metadata['source']}\\n\")\n",
    "print(f\"Taking a page from meta data from the book:\\n{docs[0].metadata['page']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generate context chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXTUAL_RAG_PROMPT = \"\"\"\n",
    "Given the document below, we want to explain what the chunk captures in the document.\n",
    "\n",
    "{WHOLE_DOCUMENT}\n",
    "\n",
    "Here is the chunk we want to explain:\n",
    "\n",
    "{CHUNK_CONTENT}\n",
    "\n",
    "Answer ONLY with a succinct explaination of the meaning of the chunk in the context of the whole document above.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_prompts(document:str,chunks):\n",
    "#     prompts = []\n",
    "#     for chunk in chunks:\n",
    "#         prompt = prompt_template.format(whole_document=document,chunk_content=chunks)\n",
    "#         prompts.append(prompt)\n",
    "#     return prompts\n",
    "\n",
    "# prompts = generate_prompts(full_document,docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_prompts(whole_document: str, chunks: str):\n",
    "    \"\"\"\n",
    "    Generates a contextual response based on the given prompt using the specified language model.\n",
    "    Args:\n",
    "        prompt (str): The input prompt to generate a response for.\n",
    "    Returns:\n",
    "        str: The generated response content from the language model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating the prompt\n",
    "    contextual_prompt = PromptTemplate(\n",
    "    input_variables = ['WHOLE_DOCUMENT','CHUNK_CONTENT'],\n",
    "    template = CONTEXTUAL_RAG_PROMPT)\n",
    "\n",
    "    # Chaining\n",
    "    # llm_chain = llm_contextual | contextual_prompt\n",
    "\n",
    "    # Create the LLMChain by chaining the prompt with the LLM\n",
    "    llm_chain = LLMChain(llm=llm_contextual, prompt=contextual_prompt)\n",
    "    \n",
    "    # Needed lists\n",
    "    contextual_chunks,all_metadatas = [],[]\n",
    "    \n",
    "    # Loop through the chunks\n",
    "    for index,chunk_context in enumerate(chunks):\n",
    "        chunk_context = chunk_context.page_content\n",
    "        whole_document = whole_document\n",
    "        input_data = {\n",
    "            \"WHOLE_DOCUMENT\":whole_document,\"CHUNK_CONTENT\":chunk_context\n",
    "        }\n",
    "\n",
    "        # Getting each response\n",
    "        contextual_response = llm_chain.invoke(input_data)\n",
    "        if contextual_response['text'] == None:\n",
    "            contextual_response['text'] = \" \"\n",
    "        # print(contextual_response.keys())\n",
    "        # print(contextual_response['text'])\n",
    "        # break\n",
    "        # Append to respective lists\n",
    "            contextual_chunks.append(contextual_response['text'].strip()+\" \"+chunk_context)\n",
    "        else:\n",
    "            contextual_chunks.append(contextual_response['text'].strip()+\" \"+chunk_context)\n",
    "        all_metadatas.append({\"source\": f\"Document_{chunks[index].metadata['page']}\"})\n",
    "    \n",
    "    # return contextual_chunks,all_metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['WHOLE_DOCUMENT', 'CHUNK_CONTENT', 'text'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate_context_prompts(full_document[:80000], docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# This taking much time due to the document being too large\n",
    "contextual_chunks,all_metadatas = generate_context_prompts(full_document[:5000], docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sowmya Vajjala,  \\nBodhisattwa Majumder,  \\nAnuj Gupta & Harshit SuranaPractical  \\nNatural Language \\nProcessing\\nA Comprehensive Guide to Building  \\nReal-World NLP Systems Praise for Practical Natural Language Processing\\nPractical NLP  focuses squarely on an overlooked demographic: the practitioners and\\nbusiness leaders in industry! While many great books focus on ML ’s algorithmic\\nfundamentals, this book exposes the anatomy of real-world systems: from e-commerce\\napplications to virtual assistants. Painting a realistic picture of modern production\\nsystems, the book teaches not only deep learning, but also the heuristics and patchwork\\npipelines that define the (actual) state of the art for deployed NLP systems. The authors\\nzoom out, teaching problem formulation, and aren’t afraid to zoom in on the grimy\\ndetails, including handling messy data and sustaining live systems. This book will prove\\ninvaluable to industry professionals keen to build and deploy NLP in the wild.\\n—Zachary Lipton, Assistant Professor, Carnegie Mellon\\nUniversity, Scientist at Amazon AI, Author of Dive into Deep Learning\\nThis book does a great job bridging the gap between natural language processing (NLP)\\nresearch and practical applications. From healthcare to e-commerce and finance, it\\ncovers many of the most sought-after domains where NLP is being put to use and\\nwalks through core tasks in a clear and understandable manner. Overall, the book\\nis a great manual on how to get the most out of current NLP in your industry.\\n—Sebastian Ruder, Research Scientist, Google DeepMind\\nThere are two kinds of computer science books on the market: academic textbooks\\nthat give you a deep understanding of a domain but can be difficult to access for a\\nnon-academic, and “cookbooks” that outline solutions to very specific problems\\nwithout providing the technical foundations that would allow the reader to generalize\\nthe recipes. This book offers the best of both worlds: it is thorough yet accessible. It\\nprovides the reader with a solid foundation in natural-language processing. . . . If\\nyou would like to go from zero to one in NLP , this book is for you!\\n—Marc Najork, Research Engineering Director, Google AI,\\nACM & IEEE Fellow There are text books or research papers or books on programming tips, but not a book\\nthat tells us how to build an end-to-end NLP system from scratch. I am happy to see\\nthis book on practical NLP , which fills this much needed gap. The authors have\\nmeticulously, thoughtfully and lucidly covered each and every aspect of NLP\\nthat one has to be aware of while building large scale practical systems; at the same\\ntime, this book has also managed to cover a large number of examples and varied\\napplication areas and verticals. This book is a must for all aspiring NLP engineers,\\nentrepreneurs who want to build companies around language technologies, and also\\nacademic researchers who would like to see their inventions reach the real users.\\n—Monojit, Principal Researcher, Microsoft  Research India,\\nAdjunct Faculty at IIIT Hyderabad, Ashoka University, IIT Kharagpur\\nThis book bridges the gap between theory and practice by explaining the underlying\\nconcepts while keeping in mind varied real-world deployments across different\\nbusiness verticals. There is much hard-fought practical advice from the trenches\\nwhether it is about tweaking parameters of open source libraries, setting up\\ndata pipelines for building models, or optimizing for fast inference.\\nA must-read for engineers building NLP applications.\\n—Vinayak Hegde, CTO-in-Residence, Microsoft  For Startups\\nThis book shows how to put NLP to practice. It bridges the gap between NLP theory and\\npractical engineering. The authors achieved a rare feat by simplifying the esoteric art\\nof design and architecture of production quality machine learning systems.\\nI wish I had access to this book early on in my professional career and evaded\\nthe mistakes I made along the way. . . . I am deeply convinced that this\\nbook is an essential read for anybody aiming to develop involved\\nin developing a robust, high-performing NLP system.\\n—Siddharth Sharma, ML Engineer, Facebook\\nI feel this is not only an essential book for NLP practitioners, it is also a valuable reference\\nfor the research community to understand the problem spaces in real-world\\napplications. I very much appreciate this book and wish this could be a\\nlong-term project with up-to-date NLP application trending!\\n—Mengting Wan, Data Scientist (ML&NLP) at Airbnb,\\nMicrosoft  Research Fellow Sowmya Vajjala, Bodhisattwa Majumder,\\nAnuj Gupta, and Harshit SuranaPractical Natural Language\\nProcessing\\nA Comprehensive Guide to Building\\nReal-World NLP Systems\\nBoston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing 978-1-492-05405-4\\n[LSI]Practical Natural Language Processing\\nby Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, and Harshit Surana\\nCopyright © 2020 Anuj Gupta, Bodhisattwa Prasad Majumder, Sowmya Vajjala, and Harshit Surana. All\\nrights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com .\\nAcquistions Editor:  Jonathan Hassell\\nDevelopmental Editor:  Melissa Potter\\nProduction Editor:  Beth Kelly\\nCopyeditor:  Holly Forsyth\\nProofreader:  Charles RoumeliotisIndexer:  nSight Inc.\\nInterior Designer:  David Futato\\nCover Designer:  Karen Montgomery\\nIllustrator:  Rebecca Demarest\\nJune 2020:  First Edition\\nRevision History for the First Edition\\n2020-06-17: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492054054  for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Practical Natural Language Processing ,\\nthe cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the authors, and do not represent the publisher’s views.\\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights. This book is dedicated to our respective advisors: Detmar Meurers, Julian McAuley,\\nKannan Srinathan, and Luis von Ahn. Table of Contents\\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xv\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xvii\\nPart I. Foundations\\n1.NLP: A Primer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nNLP in the Real World                                                                                                    5\\nNLP Tasks                                                                                                                      6\\nWhat Is Language?                                                                                                           8\\nBuilding Blocks of Language                                                                                      9\\nWhy Is NLP Challenging?                                                                                         12\\nMachine Learning, Deep Learning, and NLP: An Overview                                  14\\nApproaches to NLP                                                                                                       16\\nHeuristics-Based NLP                                                                                                16\\nMachine Learning for NLP                                                                                       19\\nDeep Learning for NLP                                                                                             22\\nWhy Deep Learning Is Not Y et the Silver Bullet for NLP                                    28\\nAn NLP Walkthrough: Conversational Agents                                                         31\\nWrapping Up                                                                                                                  33\\n2.NLP Pipeline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\\nData Acquisition                                                                                                            39\\nText Extraction and Cleanup                                                                                        42\\nHTML Parsing and Cleanup                                                                                    44\\nUnicode Normalization                                                                                             45\\nSpelling Correction                                                                                                    46\\nvii System-Specific Error Correction                                                                            47\\nPre-Processing                                                                                                                49\\nPreliminaries                                                                                                               50\\nFrequent Steps                                                                                                            52\\nOther Pre-Processing Steps                                                                                      55\\nAdvanced Processing                                                                                                 57\\nFeature Engineering                                                                                                      60\\nClassical NLP/ML Pipeline                                                                                       62\\nDL Pipeline                                                                                                                  62\\nModeling                                                                                                                         62\\nStart with Simple Heuristics                                                                                     63\\nBuilding Y our Model                                                                                                 64\\nBuilding THE Model                                                                                                 65\\nEvaluation                                                                                                                       68\\nIntrinsic Evaluation                                                                                                    68\\nExtrinsic Evaluation                                                                                                   71\\nPost-Modeling Phases                                                                                                   72\\nDeployment                                                                                                                 72\\nMonitoring                                                                                                                  72\\nModel Updating                                                                                                          73\\nWorking with Other Languages                                                                                   73\\nCase Study                                                                                                                       74\\nWrapping Up                                                                                                                  76\\n3.Text Representation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  81\\nVector Space Models                                                                                                     84\\nBasic Vectorization Approaches                                                                                  85\\nOne-Hot Encoding                                                                                                     85\\nBag of Words                                                                                                               87\\nBag of N-Grams                                                                                                          89\\nTF-IDF                                                                                                                         90\\nDistributed Representations                                                                                         92\\nWord Embeddings                                                                                                     94 Word Embeddings                                                                                                     94\\nGoing Beyond Words                                                                                              103\\nDistributed Representations Beyond Words and Characters                               105\\nUniversal Text Representations                                                                                 107\\nVisualizing Embeddings                                                                                             108\\nHandcrafted Feature Representations                                                                      112\\nWrapping Up                                                                                                                113\\nviii | Table of Contents Part II. Essentials\\n4.Text Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  119\\nApplications                                                                                                                  121\\nA Pipeline for Building Text Classification Systems                                               123\\nA Simple Classifier Without the Text Classification Pipeline                            125\\nUsing Existing Text Classification APIs                                                                126\\nOne Pipeline, Many Classifiers                                                                                  126\\nNaive Bayes Classifier                                                                                              127\\nLogistic Regression                                                                                                  131\\nSupport Vector Machine                                                                                         132\\nUsing Neural Embeddings in Text Classification                                                    134\\nWord Embeddings                                                                                                   134\\nSubword Embeddings and fastText                                                                       136\\nDocument Embeddings                                                                                          138\\nDeep Learning for Text Classification                                                                      140\\nCNNs for Text Classification                                                                                  143\\nLSTMs for Text Classification                                                                                144\\nText Classification with Large, Pre-Trained Language Models                         145\\nInterpreting Text Classification Models                                                                   147\\nExplaining Classifier Predictions with Lime                                                        148\\nLearning with No or Less Data and Adapting to New Domains                          149\\nNo Training Data                                                                                                     149\\nLess Training Data: Active Learning and Domain Adaptation                         150\\nCase Study: Corporate Ticketing                                                                               152\\nPractical Advice                                                                                                           155\\nWrapping Up                                                                                                                157\\n5.Information Extraction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  161\\nIE Applications                                                                                                             162\\nIE Tasks                                                                                                                         164\\nThe General Pipeline for IE                                                                                        165\\nKeyphrase Extraction                                                                                                  166\\nImplementing KPE                                                                                                  167\\nPractical Advice                                                                                                        168\\nNamed Entity Recognition                                                                                         169\\nBuilding an NER System                                                                                         171 Building an NER System                                                                                         171\\nNER Using an Existing Library                                                                              175\\nNER Using Active Learning                                                                                    176\\nPractical Advice                                                                                                        177\\nNamed Entity Disambiguation and Linking                                                            178\\nNEL Using Azure API                                                                                             179\\nTable of Contents | ix Relationship Extraction                                                                                              181\\nApproaches to RE                                                                                                     182\\nRE with the Watson API                                                                                         184\\nOther Advanced IE Tasks                                                                                           185\\nTemporal Information Extraction                                                                         186\\nEvent Extraction                                                                                                       187\\nTemplate Filling                                                                                                        189\\nCase Study                                                                                                                     190\\nWrapping Up                                                                                                                193\\n6.Chatbots. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  197\\nApplications                                                                                                                  198\\nA Simple FAQ Bot                                                                                                    199\\nA Taxonomy of Chatbots                                                                                            200\\nGoal-Oriented Dialog                                                                                              202\\nChitchats                                                                                                                    202\\nA Pipeline for Building Dialog Systems                                                                   203\\nDialog Systems in Detail                                                                                             204\\nPizzaStop Chatbot                                                                                                    206\\nDeep Dive into Components of a Dialog System                                                    216\\nDialog Act Classification                                                                                         217\\nIdentifying Slots                                                                                                        217\\nResponse Generation                                                                                               218\\nDialog Examples with Code Walkthrough                                                           219\\nOther Dialog Pipelines                                                                                                224\\nEnd-to-End Approach                                                                                             225\\nDeep Reinforcement Learning for Dialogue Generation                                   225\\nHuman-in-the-Loop                                                                                                226\\nRasa NLU                                                                                                                      227\\nA Case Study: Recipe Recommendations                                                                230\\nUtilizing Existing Frameworks                                                                               231\\nOpen-Ended Generative Chatbots                                                                        233\\nWrapping Up                                                                                                                234\\n7.Topics in Brief. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  239 7.Topics in Brief. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  239\\nSearch and Information Retrieval                                                                             241\\nComponents of a Search Engine                                                                            243\\nA Typical Enterprise Search Pipeline                                                                    246\\nSetting Up a Search Engine: An Example                                                             247\\nA Case Study: Book Store Search                                                                          249\\nTopic Modeling                                                                                                            250\\nTraining a Topic Model: An Example                                                                   254\\nx | Table of Contents What’s Next?                                                                                                             255\\nText Summarization                                                                                                    256\\nSummarization Use Cases                                                                                       256\\nSetting Up a Summarizer: An Example                                                                257\\nPractical Advice                                                                                                        258\\nRecommender Systems for Textual Data                                                                 260\\nCreating a Book Recommender System: An Example                                       261\\nPractical Advice                                                                                                        262\\nMachine Translation                                                                                                   263\\nUsing a Machine Translation API: An Example                                                  264\\nPractical Advice                                                                                                        265\\nQuestion-Answering Systems                                                                                    266\\nDeveloping a Custom Question-Answering System                                          268\\nLooking for Deeper Answers                                                                                  268\\nWrapping Up                                                                                                                269\\nPart III. Applied\\n8.Social Media. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  275\\nApplications                                                                                                                  277\\nUnique Challenges                                                                                                       278\\nNLP for Social Data                                                                                                     284\\nWord Cloud                                                                                                              284\\nTokenizer for SMTD                                                                                                286\\nTrending Topics                                                                                                        286\\nUnderstanding Twitter Sentiment                                                                         288\\nPre-Processing SMTD                                                                                             290\\nText Representation for SMTD                                                                              294\\nCustomer Support on Social Channels                                                                 297\\nMemes and Fake News                                                                                                299\\nIdentifying Memes                                                                                                   299\\nFake News                                                                                                                  300\\nWrapping Up                                                                                                                302\\n9.E-Commerce and Retail. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  307\\nE-Commerce Catalog                                                                                                  308\\nReview Analysis                                                                                                        308 Review Analysis                                                                                                        308\\nProduct Search                                                                                                         309\\nProduct Recommendations                                                                                    309\\nSearch in E-Commerce                                                                                               309\\nBuilding an E-Commerce Catalog                                                                            312\\nTable of Contents | xi Attribute Extraction                                                                                                 312\\nProduct Categorization and Taxonomy                                                                317\\nProduct Enrichment                                                                                                321\\nProduct Deduplication and Matching                                                                  323\\nReview Analysis                                                                                                           324\\nSentiment Analysis                                                                                                   325\\nAspect-Level Sentiment Analysis                                                                           327\\nConnecting Overall Ratings to Aspects                                                                329\\nUnderstanding Aspects                                                                                           330\\nRecommendations for E-Commerce                                                                        332\\nA Case Study: Substitutes and Complements                                                      333\\nWrapping Up                                                                                                                336\\n10. Healthcare, Finance, and Law. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  339\\nHealthcare                                                                                                                     339\\nHealth and Medical Records                                                                                  341\\nPatient Prioritization and Billing                                                                           342\\nPharmacovigilance                                                                                                   342\\nClinical Decision Support Systems                                                                        342\\nHealth Assistants                                                                                                      342\\nElectronic Health Records                                                                                      344\\nMental Healthcare Monitoring                                                                              353\\nMedical Information Extraction and Analysis                                                    355\\nFinance and Law                                                                                                          358\\nNLP Applications in Finance                                                                                 360\\nNLP and the Legal Landscape                                                                                363\\nWrapping Up                                                                                                                366\\nPart IV. Bringing It All Together\\n11. The End-to-End NLP Process. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  371\\nRevisiting the NLP Pipeline: Deploying NLP Software                                         372\\nAn Example Scenario                                                                                              374\\nBuilding and Maintaining a Mature System                                                            376\\nFinding Better Features                                                                                           377\\nIterating Existing Models                                                                                        378\\nCode and Model Reproducibility                                                                          379\\nTroubleshooting and Interpretability                                                                    379 Troubleshooting and Interpretability                                                                    379\\nMonitoring                                                                                                                382\\nMinimizing Technical Debt                                                                                    383\\nAutomating Machine Learning                                                                              384\\nxii | Table of Contents The Data Science Process                                                                                           388\\nThe KDD Process                                                                                                     388\\nMicrosoft Team Data Science Process                                                                   390\\nMaking AI Succeed at Y our Organization                                                                392\\nTeam                                                                                                                           392\\nRight Problem and Right Expectations                                                                 393\\nData and Timing                                                                                                      394\\nA Good Process                                                                                                        395\\nOther Aspects                                                                                                           396\\nPeeking over the Horizon                                                                                           398\\nFinal Words                                                                                                                  401\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  407\\nTable of Contents | xiii Foreword\\nThe field of natural language processing (NLP) has undergone a dramatic shift in\\nrecent years, both in terms of methodology and in terms of the applications sup‐\\nported. Methodological advances have ranged from new ways of representing docu‐\\nments to new techniques for language synthesis. With these have come new\\napplications ranging from open-ended conversational systems to techniques that use\\nnatural language for model interpretability. Finally, these advances have seen NLP\\ngain a foothold in related areas, such as computer vision and recommender systems,\\nsome of which my lab is working on with support from Amazon, Samsung, and the\\nNational Science Foundation.\\nAs NLP is expanding into these exciting new areas, so too has the audience of practi‐\\ntioners wanting to make use of NLP techniques. In the Data Science course (CSE 258)\\nthat I take at the University of California–San Diego, which is often the most attended\\nin the computer science department, I see that more and more students are doing\\ntheir projects on NLP-based topics. NLP is rapidly becoming a necessary skill\\nrequired by engineers, product managers, scientists, students, and enthusiasts wish‐\\ning to build applications on top of natural language data. On one hand, new tools and\\nlibraries for NLP and machine learning have made natural language modeling more\\naccessible than ever. But on the other hand, resources for learning NLP must target\\nthis ever-growing and diverse audience. This is especially true for organizations that\\nhave recently adopted NLP or for students working with natural language data for the\\nfirst time.\\nIt has been my pleasure over the last few years to collaborate with Bodhisattwa\\nMajumder on exciting new applications in NLP and dialog, so I was thrilled to hear\\nabout his efforts (along with Sowmya Vajjala, Anuj Gupta, and Harshit Surana) to\\nwrite a book on NLP . They have a wide experience in scaling NLP including at early-\\nstage startups, the MIT Media Lab, Microsoft Research, and Google AI.\\nI am excited by the end-to-end approach taken in their book, which will make it use‐\\nful for a range of scenarios and will help readers to work with the labyrinth of\\nxv possible  options while building NLP applications. I am especially thrilled about the\\nemphasis on modern NLP applications such as chatbots, as well as the focus on inter‐\\ndisciplinary topics such as ecommerce and retail. These topics will be especially use‐\\nful for industry leaders and researchers, and are critical subjects that have been given\\nonly limited coverage in existing textbooks. This book is ideal both as a first resource\\nto discover the field of natural language processing and a guide for seasoned practi‐\\ntioners looking to discover the latest developments in this exciting area.\\n— Julian McAuley\\nProfessor of Computer Science and Engineering\\nUniversity of California, San Diego\\nxvi | Foreword Preface\\nNatural language processing (NLP) is a field at the intersection of computer science,\\nartificial intelligence, and linguistics. It concerns building systems that can process\\nand understand human language. Since its inception in the 1950s and until very\\nrecently, NLP has primarily been the domain of academia and research labs, requir‐\\ning long formal education and training. The past decade’s breakthroughs have resul‐\\nted in NLP being increasingly used in a range of diverse domains such as retail,\\nhealthcare, finance, law, marketing, human resources, and many more. There are a\\nrange of driving forces for these developments:\\n•Widely available and easy-to-use NLP tools, techniques, and APIs are now all-\\npervading in the industry. There has never been a better time to build quick NLP\\nsolutions.\\n•Development of more interpretable and generalized approaches has improved\\nthe baseline performance for even complex NLP tasks, such as open-domain\\nconversational tasks and question answering, which were not practically feasible\\nbefore.\\n•More and more organizations, including Google, Microsoft, and Amazon, are\\ninvesting heavily in more interactive consumer products, where language is used\\nas the primary medium of communication.\\n•Increased availability of useful open source datasets, along with standard bench‐\\nmarks on them, has acted as a catalyst in this revolution, as opposed to being\\nimpeded by proprietary datasets only available to limited organizations and\\nindividuals.\\n•The viability of NLP has moved beyond English or other major languages. Data‐\\nsets and language-specific models are being created for the less-frequently digi‐\\ntized languages too. A fruitful product that came out this effort was a near-\\nperfect automatic machine translation tool available to all individuals with a\\nsmartphone.\\nxvii With this rapidly expanding usage, a growing proportion of the workforce that builds\\nthese NLP systems is grappling with limited experience and theoretical knowledge\\nabout the topic. This book addresses this need from an applied perspective. Our book\\naims to guide the readers to build, iterate, and scale NLP systems in a business set‐\\nting, and to tailor them for various industry verticals.\\nWhy We Wrote This Book\\nThere are many popular books on NLP available. While some of these serve as text‐\\nbooks, focusing on theoretical aspects, some others aim to introduce NLP concepts\\nthrough a lot of code examples. There are a few others that focus on specific NLP or\\nmachine learning libraries and provide “how-to” guides on solving different NLP\\nproblems using the libraries. So, why do we need another book on NLP?\\nWe have been building and scaling NLP solutions for over a decade at leading univer‐\\nsities and technology companies. While mentoring colleagues and other engineers,\\nwe noticed a gap between NLP practice in the industry and the NLP skill sets of new\\nengineers and those who are just starting with NLP in particular. We started under‐\\nstanding these gaps even better during NLP workshops we were conducting for\\nindustry professionals, where we noticed that business and engineering leaders also\\nhave these gaps.\\nMost online courses and books tackle NLP problems using toy use cases and popular\\n(often large, clean, and well-defined) datasets. While this imparts the general meth‐\\nods of NLP , we believe it does not provide enough of a foundation to tackle new\\nproblems and develop specific solutions in the real world. Commonly encountered\\nproblems while building real-world applications, such as data collection, working\\nwith noisy data and signals, incremental development of solutions, and issues\\ninvolved in deploying the solutions as a part of a larger application, are not dealt with\\nby existing resources, to the best of our knowledge. We also saw that best practices to\\ndevelop NLP systems were missing in most scenarios. We felt a book was needed to\\nbridge this gap, and that is how this book was born!\\nThe Philosophy\\nWe want to provide a holistic and practical perspective that enables the reader to suc‐\\ncessfully build real-world NLP solutions embedded in larger product setups. Thus,\\nmost chapters are accompanied by code walkthroughs in the associated Git reposi‐\\ntory. The book is also supplemented with extensive references for readers who want\\nto delve deeper. Throughout the book, we start with a simple solution and incremen‐\\ntally build more complex solutions by taking a minimum viable product (MVP)\\napproach, as commonly found in industry practice. We also give tips based on our\\nexperience and learnings. Where possible, each chapter is accompanied by a\\nxviii | Preface discussion  on the state-of-the-art in that topic. Most chapters conclude with a case\\nstudy of real-world use cases.\\nConsider the task of building a chatbot or text classification system at your organiza‐\\ntion. In the beginning there may be little or no data to work with. At this point, a\\nbasic solution using rule-based systems or traditional machine learning will be apt.\\nAs you accumulate more data, more sophisticated NLP techniques (which are often\\ndata intensive) can be used, including deep learning. At each step of this journey\\nthere are dozens of alternative approaches one can take. This book will help you navi‐\\ngate this maze of options.\\nScope\\nThis book provides a comprehensive view on building real-world NLP applications.\\nWe will cover the complete lifecycle of a typical NLP project—from data collection to\\ndeploying and monitoring the model. Some of these steps are applicable to any ML\\npipeline, while some are very specific to NLP . We also introduce task-specific case\\nstudies and domain-specific guides to build an NLP system from scratch. We specifi‐\\ncally cover a gamut of tasks ranging from text classification to question answering to\\ninformation extraction and dialog systems. Similarly, we provide recipes to apply\\nthese tasks in domains ranging from e-commerce to healthcare, social media, and\\nfinance. Owing to the depth and breadth of the topics and scenarios we cover, we will\\nnot go step by step explaining the code and all the concepts. For details of the imple‐\\nmentation, we have provided detailed source code notebooks. The code snippets in\\nthis book cover the core logic and often skip introductory steps like setting up a\\nlibrary or importing a package as they are covered in the associated notebooks. To\\ncover the wide range of concepts we have given more than 450 extensive references to\\ndelve deeper into these topics. This book will be a day-to-day cookbook giving you a\\npragmatic view while building any NLP system, as well as be a stepping stone to\\nbroaden the application of NLP into your domain.\\nWho Should Read This Book\\nThis book is for anyone involved in building NLP applications for real-world use\\ncases. This includes software developers and testers, machine learning engineers, data\\nengineers, MLOps engineers, NLP engineers, data scientists, product managers, peo‐\\nple managers, VPs, CXOs, and startup founders. This also includes those involved in\\ndata creation and annotation processes—in short anyone and everyone who is\\ninvolved in any way in building NLP systems in industry. While not all chapters are\\nuseful for people with all roles, we tried to give lucid explanations using less technical\\njargon and more intuitive understanding wherever possible. We believe there is\\nsomething in every chapter for all potential readers interested in getting a holistic\\nperspective about building NLP applications.\\nPreface | xix Some chapters or sections can be understood without much coding experience and\\ncode bits can be skipped as needed. For example, the first two sections in Chapter 1\\nand Chapter 9, or the sections “The Data Science Process” and “Making AI succeed in\\nyour organization” in Chapter 11 can be understood without any coding experience\\nby all groups of readers. As you progress through the book, you will find more such\\nsections in all chapters. However, to extract maximum benefit from this book, its\\nnotebooks, and references, we expect the reader to have the following background:\\n•Intermediate proficiency in Python programming. For example, understanding\\nPython features such as list comprehension, writing functions and classes, and\\nusing existing libraries.\\n•Familiarity with various aspects of the software development life cycle (SDLC)\\nsuch as design, development, testing, DevOps, etc.\\n•Basics of machine learning, including familiarity with commonly used machine\\nlearning algorithms such as logistic regression and decision trees and the ability\\nto use them in Python with existing libraries such as scikit-learn.\\n•Basic knowledge of NLP is useful but not mandatory. Having an idea of tasks\\nsuch as text classification and named entity recognition is also helpful.\\nWhat You Will Learn\\nOur primary audience is comprised of engineers and scientists involved in building\\nreal-world NLP systems for different verticals. Some of the common job titles are:\\nSoftware Engineer, NLP Engineer, ML Engineer, and Data Scientist. The book may\\nalso be helpful for product managers and engineering leaders. However, it may not be\\nas helpful for those pursuing cutting-edge research in NLP because we do not cover\\nin-depth theoretical and technical details related to NLP concepts. With this book,\\nyou will:\\n•Understand the wide spectrum of problem statements, tasks, and solution\\napproaches within NLP .\\n•Gain experience in implementing and evaluating different NLP applications and\\napplying machine learning and deep learning methods for this process.\\n•Fine-tune an NLP solution based on the business problem and industry vertical.\\n•Evaluate various algorithms and approaches for the given task, dataset, and stage\\nof the NLP product.\\n•Plan the lifecycle of the NLP product and produce software solutions following\\nbest practices around release, deployment, and DevOps for NLP systems.\\nxx | Preface •Understand best practices, opportunities, and the roadmap for NLP from a busi‐\\nness and product leader’s perspective.\\nY ou will also learn to adapt your solutions for different industry verticals like health‐\\ncare, finance, and retail. Moreover, you will learn about specific caveats you will\\nencounter in each.\\nStructure of the Book\\nThe book is divided into four sections. Figure P-1  illustrates the chapter organization.\\nIsolated chapters that are not directly connected to other chapters are easiest to skip\\nwhile moving forward.\\nFigure P-1. How the book’s sections are structured\\nPart I, Foundations  acts as a bedrock for the rest of the book, by giving an overview of\\nNLP ( Chapter 1 ), discussing typical data processing and modeling pipelines used in\\nbuilding NLP systems ( Chapter 2 ), and introducing different ways of representing\\ntextual data in NLP ( Chapter 3 ).\\nPreface | xxi Part II, Essentials , focuses on the most common NLP applications, with an emphasis\\non real-world use cases. Where possible, we show multiple solutions to the problem\\nat hand to demonstrate how to choose among different options. Some applications\\ninclude text classification ( Chapter 4 ), information extraction ( Chapter 5 ), and the\\nbuilding of chat bots ( Chapter 6 ). We also introduce other applications, such as\\nsearch, topic modeling, text summarization, and machine translation, along with a\\ndiscussion about practical use cases ( Chapter 7 ).\\nPart III, Applied  (Chapters 8–10) specifically focuses on three industry verticals where\\nNLP is heavily used, with a detailed discussion on those domains’ specific problems\\nand how NLP is useful in addressing them.\\nFinally, Part IV  (Chapter 11 ) brings all the learning together by dealing with the\\nissues involved in end-to-end deployment of NLP systems in practice.\\nHow to Read This Book\\nHow one reads the book depends on their role and objective. For a data scientist or\\nan engineer delving into NLP , we recommend reading Chapters 1–6 and then focus‐\\ning on the particular domain or subproblem of interest. For someone in a leadership\\nrole, we recommend focusing on Chapters 1, 2, and 11. They might want to give extra\\nfocus to case studies for Chapters 3–7, which provide more ideas on the process of\\nbuilding NLP applications from scratch. A product leader might want to delve deep\\ninto the references provided for relevant chapters, as well as Chapter 11 .\\nNLP applications for various domains can be different from the general problems\\ncovered in Chapters 3–7. That is why we have focused more on certain domains such\\nas e-commerce, social media, healthcare, finance, and law. If your interest or work\\ntakes you to these areas, you can dig deeper into those chapters and corresponding\\nreferences.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nxxii | Preface Constant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nThis element signifies a tip or suggestion.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nUsing Code Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://oreil.ly/PracticalNLP .\\nIf you have a technical question or a problem using the code examples, please send\\nemail to bookquestions@oreilly.com .\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing examples from O’Reilly\\nbooks does require permission. Answering a question by citing this book and quoting\\nexample code does not require permission. Incorporating a significant amount of\\nexample code from this book into your product’s documentation does require\\npermission.\\nWe appreciate, but generally do not require, attribution. An attribution usually\\nincludes the title, author, publisher, and ISBN. For example: “ Practical Natural Lan‐\\nguage Processing  by Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, and Har‐\\nshit Surana (O’Reilly). Copyright 2020 Anuj Gupta, Bodhisattwa Prasad Majumder,\\nSowmya Vajjala, and Harshit Surana, 978-1-492-05405-4. ”\\nPreface | xxiii If you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com .\\nO’Reilly Online Learning\\nFor more than 40 years, O’Reilly Media  has provided technol‐\\nogy and business training, knowledge, and insight to help\\ncompanies succeed.\\nOur unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\\nplatform gives you on-demand access to live training courses, in-depth learning\\npaths, interactive coding environments, and a vast collection of text and video from\\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at https://oreil.ly/PNLP .\\nEmail bookquestions@oreilly.com  to comment or ask technical questions about this\\nbook.\\nFor news and information about our books and courses, visit http://oreilly.com .\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://youtube.com/oreillymedia\\nFurther Information\\nIn the GitHub repo  you will find notebooks explaining various concepts covered in\\nthe book. The notebooks have been organized by chapter. We also provide additional\\nnotebooks, not necessarily covered in the book.\\nxxiv | Preface Book website: http://www.practicalnlp.ai\\nThe world of NLP is always evolving. To stay updated on how concept mentioned in\\nthe book fit into the broader context one, two, and five years from now, follow our\\nblog . We keep it updated with relevant writeups and articles, and tag every post with\\nthe book’s corresponding chapter title.\\nContact the authors:\\nEmail: authors@practicalnlp.ai\\nLinkedin: https://linkedin.com/company/practical-nlp\\nTwitter: https://twitter.com/PracticalNLProc\\nFacebook: https://oreil.ly/facebookPNLP\\nAcknowledgments\\nA book like this is a compendium of knowledge; hence, it cannot exist in isolation.\\nWhile writing this book, we drew a lot of inspiration and information from several\\nbooks, research papers, software projects and numerous other resources on the inter‐\\nnet. We thank the NLP and Machine Learning community for all their efforts. We\\nhave merely stood on the shoulders on these giants. We also thank various people\\nwho attended some of the authors’ talks and workshops and participated in discus‐\\nsions that lead to the idea of writing this book and shaping its premise. This book is\\nthe result of a long collaborative effort and several people supported us in different\\nways in our endeavor.\\nWe thank the O’Reilly reviewers Will Scott, Darren Cook, Ramya Balasubramaniam,\\nPriyanka Raghavan, and Siddharth Narayanan for their meticulous, invaluable, and\\ndetailed comments which helped us improve earlier drafts. Detailed feedback from\\nSiddharth Sharma, Sumod Mohan, Vinayak Hegde, Aasish Pappu, Taranjeet Singh,\\nKartikay Bagla, and Varun Purushotham were instrumental in improving the quality\\nof the content.\\nWe are also very thankful to Rui Shu, Shreyans Dhankhar, Jitin Kapila, Kumarjit\\nPathak, Ernest Kirubakaran Selvaraj, Robin Singh, Ayush Datta, Vishal Gupta, and\\nNachiketh for helping us prepare the early versions of code notebooks. We would\\nespecially like to thank Varun Purushotham, who spent several weeks reading and\\nrereading our drafts and preparing and cross-checking the code notebooks. This\\nbook would not be the same without his contribution.\\nWe would like to thank the O’Reilly Media team, without whom this would not have\\nbeen possible: Jonathan Hassell, for giving us this opportunity; Melissa Potter, for\\nregularly following up with us throughout this journey and patiently answering all\\nPreface | xxv our questions! Beth Kelly and Holly Forsyth, for all the help and support in shaping it\\ninto a book from the chapter drafts.\\nFinally, the following are personal thank you notes by each author:\\nSowmya : My first and biggest thank you goes to my daughter, Sahasra Malathi, whose\\nbirth and first year of life coincided with the writing of this book. It is not easy to\\nwrite a book, and not easy at all to write it with a newborn. And yet, here we are.\\nThank you, Sahasra! My mom, Geethamani, and my husband, Sriram, supported my\\nwriting by taking up baby care and household duties at different phases of writing.\\nMy friends, Purnima and Visala, were always available to listen to my excited updates\\nas well as rants about the book. My boss, Cyril Goutte, encouraged me and checked\\non my writing progress throughout. Discussions with my former colleagues, Chris\\nCardinal and Eric Le Fort, taught me a lot about developing NLP solutions for indus‐\\ntry problems, without which I perhaps would never have thought of being a part of\\nthis kind of book. I thank all of them for their support.\\nBodhisattwa : I would like to take this opportunity to thank my parents, their unques‐\\ntionable sacrifice, and the constant encouragement that made me the person I am\\ntoday. Their efforts have instilled in me the love and dedication to learning in my life.\\nI am eternally grateful to my advisors, Prof. Animesh Mukherjee and Pawan Goyal,\\nwho introduced me to this world of NLP; and Prof. Julian McAuley, who is nothing\\nless than fundamental to my technical, academic, and personal development in my\\nPhD career. The courses taken by my other professors—Taylor Berg-Kirkpatrick,\\nLawrence Saul, David Kriegman, Debasis Sengupta, Sudeshna Sarkar, and Sourav Sen\\nGupta—have significantly shaped my learning on the subject. In the early days of the\\nbook, my colleagues from Walmart Labs—especially Subhasish Misra, Arunita Das,\\nSmaranya Dey, Sumanth Prabhu, and Rajesh Bhat—gave me the motivation for this\\ncrazy idea. To my mentors at Google AI, Microsoft Research, Amazon Alexa, and my\\nlabmates from the UCSD NLP Group, thank you for being supportive and helpful in\\nthis entire journey. Also, my friends Sanchaita Hazra, Sujoy Paul, and Digbalay Bose,\\nwho stood by me through thick and thin in this mammoth project, deserve a special\\nmention. At last, none of this would have been possible without my coauthors, who\\nbelieved in this project and stayed together till the last bit of it!\\nAnuj : First and foremost, I would like to express my sincere gratitude to my wife,\\nAnu, and my son, Nirvaan. Without their unwavering support, I would not have been\\nable to devote the last three years to this endeavor. Thank you so much! I would also\\nlike to thank my parents and family for their encouragement. A big shout out goes to\\nSaurabh Arora, for introducing me to the world of NLP . Many thanks to my friends,\\nthe late Vivek Jain and Mayur Hemani; they have always encouraged me to keep\\ngoing, especially in hard times. I would also like to thank all of the amazing people\\ninvolved in machine learning communities in Bangalore; especially: Sumod Mohan,\\nVijay Gabale, Nishant Sinha, Ashwin Kumar, Mukundhan Srinivasan, Zainab Bawa,\\nxxvi | Preface and Naresh Jain for all the wonderful and thought-provoking discussions. I would\\nlike to thank my colleagues—former and present—at CSTAR, Airwoot, FreshWorks,\\nHuawei Research, Intuit, and Vahan, Inc., for everything they taught me. To my pro‐\\nfessors, Kannan Srinathan, P .R.K Rao, and B. Y egnanarayana, whose teachings have\\nhad a profound impact on me.\\nHarshit : I want to thank my parents, who have supported and encouraged me to pur‐\\nsue every crazy idea I have had. I cannot thank my dear friends Preeti Shrimal and\\nDev Chandan enough. They have been with me throughout the book’s entire journey.\\nTo my cofounders, Abhimanyu Vyas and Aviral Mathur, thank you for adjusting our\\nstartup endeavor to help me complete the book. I want to thank all my former collea‐\\ngues at Quipio and Notify.io who helped crystalize my thinking, especially Zubin\\nWadia, Amit Kumar, and Naveen Koorakula. None of this would have been possible\\nwithout my professors and everything they taught me—thank you, Prof. Luis von\\nAhn, Anil Kumar Singh, Alan W Black, William Cohen, Lori Levin, and Carlos\\nGuestrin. I also want to acknowledge Kaustuv DeBiswas, Siddharth Narayanan, Sid‐\\ndharth Sharma, Alok Parlikar, Nathan Schneider, Aasish Pappu, Manish Jawa, Sumit\\nPandey, and Mohit Ranka, who have supported me at various junctures of this\\njourney.\\nPreface | xxvii PART I\\nFoundations CHAPTER 1\\nNLP: A Primer\\nA language is not just words. It’s a culture, a tradition,\\na unification  of a community,\\na whole history that creates what a community is.\\nIt’s all embodied in a language.\\n—Noam Chomsky\\nImagine a hypothetical person, John Doe. He’s the CTO of a fast-growing technology\\nstartup. On a busy day, John wakes up and has this conversation with his digital\\nassistant:\\nJohn:  “How is the weather today?”\\nDigital assistant:  “It is 37 degrees centigrade outside with no rain today. ”\\nJohn:  “What does my schedule look like?”\\nDigital assistant:  “Y ou have a strategy meeting at 4 p.m. and an all-hands at 5:30 p.m.\\nBased on today’s traffic situation, it is recommended you leave for the office by\\n8:15 a.m. ”\\nWhile he’s getting dressed, John probes the assistant on his fashion choices:\\nJohn:  “What should I wear today?”\\nDigital assistant:  “White seems like a good choice. ”\\nY ou might have used smart assistants such as Amazon Alexa , Google Home, or Apple\\nSiri to do similar things. We talk to these assistants not in a programming language,\\nbut in our natural language—the language we all communicate in. This natural lan‐\\nguage has been the primary medium of communication between humans since time\\nimmemorial. But computers can only process data in binary, i.e., 0s and 1s. While we\\ncan represent language data in binary, how do we make machines understand the\\n3 language?  This is where natural language processing (NLP) comes in. It is an area of\\ncomputer science that deals with methods to analyze, model, and understand human\\nlanguage. Every intelligent application involving human language has some NLP\\nbehind it. In this book, we’ll explain what NLP is as well as how to use NLP to build\\nand scale intelligent applications. Due to the open-ended nature of NLP problems,\\nthere are dozens of alternative approaches one can take to solve a given problem. This\\nbook will help you navigate this maze of options and suggests how to choose the best\\noption based on your problem.\\nThis chapter aims to give a quick primer of what NLP is before we start delving\\ndeeper into how to implement NLP-based solutions for different application scenar‐\\nios. We’ll start with an overview of numerous applications of NLP in real-world sce‐\\nnarios, then cover the various tasks that form the basis of building different NLP\\napplications. This will be followed by an understanding of language from an NLP\\nperspective and of why NLP is difficult. After that, we’ll give an overview of heuris‐\\ntics, machine learning, and deep learning, then introduce a few commonly used algo‐\\nrithms in NLP . This will be followed by a walkthrough of an NLP application. Finally,\\nwe’ll conclude the chapter with an overview of the rest of the topics in the book.\\nFigure 1-1  shows a preview of the organization of the chapters in terms of various\\nNLP tasks and applications.\\nFigure 1-1. NLP tasks and applications\\nLet’s start by taking a look at some popular applications you use in everyday life that\\nhave some form of NLP as a major component.\\n4 | Chapter 1: NLP: A Primer NLP in the Real World\\nNLP is an important component in a wide range of software applications that we use\\nin our daily lives. In this section, we’ll introduce some key applications and also take a\\nlook at some common tasks that you’ll see across different NLP applications. This\\nsection reinforces the applications we showed you in Figure 1-1 , which you’ll see in\\nmore detail throughout the book.\\nCore applications:\\n•Email platforms, such as Gmail, Outlook, etc., use NLP extensively to provide a\\nrange of product features, such as spam classification, priority inbox, calendar\\nevent extraction, auto-complete, etc. We’ll discuss some of these in detail in\\nChapters 4 and 5.\\n•Voice-based assistants, such as Apple Siri, Google Assistant , Microsoft Cortana ,\\nand Amazon Alexa rely on a range of NLP techniques to interact with the user,\\nunderstand user commands, and respond accordingly. We’ll cover key aspects of\\nsuch systems in Chapter 6 , where we discuss chatbots.\\n•Modern search engines, such as Google and Bing, which are the cornerstone of\\ntoday’s internet, use NLP heavily for various subtasks, such as query understand‐\\ning, query expansion, question answering, information retrieval, and ranking\\nand grouping of the results, to name a few. We’ll discuss some of these subtasks\\nin Chapter 7 .\\n•Machine translation services, such as Google Translate, Bing Microsoft Transla‐\\ntor, and Amazon Translate are increasingly used in today’s world to solve a wide\\nrange of scenarios and business use cases. These services are direct applications\\nof NLP . We’ll touch on machine translation in Chapter 7 .\\nOther applications:\\n•Organizations across verticals analyze their social media feeds to build a better\\nand deeper understanding of the voice of their customers. We’ll cover this in\\nChapter 8 .\\n•NLP is widely used to solve diverse sets of use cases on e-commerce platforms\\nlike Amazon. These vary from extracting relevant information from product\\ndescriptions to understanding user reviews. Chapter 9  covers these in detail.\\n•Advances in NLP are being applied to solve use cases in domains such as health‐\\ncare, finance, and law. Chapter 10  addresses these.\\n•Companies such as Arria [ 1] are working to use NLP techniques to automatically\\ngenerate reports for various domains, from weather forecasting to financial\\nservices.\\nNLP in the Real World | 5 •NLP forms the backbone of spelling- and grammar-correction tools, such as \\nGrammarly and spell check in Microsoft Word and Google Docs.\\n•Jeopardy!  is a popular quiz show on TV . In the show, contestants are presented\\nwith clues in the form of answers, and the contestants must phrase their respon‐\\nses in the form of questions. IBM  built the Watson AI to compete with the show’s\\ntop players. Watson won the first prize with a million dollars, more than the\\nworld champions. Watson AI was built using NLP techniques and is one of the\\nexamples of NLP bots winning a world competition.\\n•NLP is used in a range of learning and assessment tools  and technologies, such as\\nautomated scoring in exams like the Graduate Record Examination (GRE), plagi‐\\narism detection (e.g., Turnitin), intelligent tutoring systems, and language learn‐\\ning apps like Duolingo.\\n•NLP is used to build large knowledge bases, such as the Google Knowledge\\nGraph, which are useful in a range of applications like search and question\\nanswering.\\nThis list is by no means exhaustive. NLP is increasingly being used across several\\nother applications, and newer applications of NLP are coming up as we speak. Our\\nmain focus is to introduce you to the ideas behind building these applications. We do\\nso by discussing different kinds of NLP problems and how to solve them. To get a\\nperspective on what you are about to learn in this book, and to appreciate the nuan‐\\nces that go into building these NLP applications, let’s take a look at some key NLP\\ntasks that form the bedrock of many NLP applications and industry use cases.\\nNLP Tasks\\nThere is a collection of fundamental tasks that appear frequently across various NLP\\nprojects. Owing to their repetitive and fundamental nature, these tasks have been\\nstudied extensively. Having a good grip on them will make you ready to build various\\nNLP applications across verticals. (We also saw some of these tasks earlier in\\nFigure 1-1 .) Let’s briefly introduce them:\\nLanguage modeling\\nThis is the task of predicting what the next word in a sentence will be based on\\nthe history of previous words. The goal of this task is to learn the probability of a\\nsequence of words appearing in a given language. Language modeling is useful\\nfor building solutions for a wide variety of problems, such as speech recognition,\\noptical character recognition, handwriting recognition, machine translation, and\\nspelling correction.\\n6 | Chapter 1: NLP: A Primer Text classification\\nThis is the task of bucketing the text into a known set of categories based on its\\ncontent. Text classification is by far the most popular task in NLP and is used in a\\nvariety of tools, from email spam identification to sentiment analysis.\\nInformation extraction\\nAs the name indicates, this is the task of extracting relevant information from\\ntext, such as calendar events from emails or the names of people mentioned in a\\nsocial media post.\\nInformation retrieval\\nThis is the task of finding documents relevant to a user query from a large collec‐\\ntion. Applications like Google Search are well-known use cases of information\\nretrieval.\\nConversational agent\\nThis is the task of building dialogue systems that can converse in human lan‐\\nguages. Alexa, Siri, etc., are some common applications of this task.\\nText summarization\\nThis task aims to create short summaries of longer documents while retaining the\\ncore content and preserving the overall meaning of the text.\\nQuestion answering\\nThis is the task of building a system that can automatically answer questions\\nposed in natural language.\\nMachine translation\\nThis is the task of converting a piece of text from one language to another. Tools\\nlike Google Translate are common applications of this task.\\nTopic modeling\\nThis is the task of uncovering the topical structure of a large collection of docu‐\\nments. Topic modeling is a common text-mining tool and is used in a wide range\\nof domains, from literature to bioinformatics.\\nFigure 1-2  shows a depiction of these tasks based on their relative difficulty in terms\\nof developing comprehensive solutions.\\nNLP in the Real World | 7 Figure 1-2. NLP tasks organized according to their relative difficulty\\nIn the rest of the chapters in this book, we’ll see these tasks’ challenges and learn how\\nto develop solutions that work for certain use cases (even the hard tasks shown in the\\nfigure). To get there, it is useful to have an understanding of the nature of human lan‐\\nguage and the challenges in automating language processing. The next two sections\\nprovide a basic overview.\\nWhat Is Language?\\nLanguage is a structured system of communication that involves complex combina‐\\ntions of its constituent components, such as characters, words, sentences, etc. Lin‐\\nguistics is the systematic study of language. In order to study NLP , it is important to\\nunderstand some concepts from linguistics about how language is structured. In this\\nsection, we’ll introduce them and cover how they relate to some of the NLP tasks we\\nlisted earlier.\\nWe can think of human language as composed of four major building blocks: pho‐\\nnemes, morphemes and lexemes, syntax, and context. NLP applications need knowl‐\\nedge of different levels of these building blocks, starting  from the basic sounds\\nof language (phonemes) to texts with some meaningful expressions (context).\\n8 | Chapter 1: NLP: A Primer Figure 1-3  shows these building blocks of language, what they encompass, and a few\\nNLP applications we introduced earlier that require this knowledge. Some of the\\nterms listed here that were not introduced earlier in this chapter (e.g., parsing, word\\nembeddings, etc.) will be introduced later in these first three chapters.\\nFigure 1-3. Building blocks of language and their applications\\nBuilding Blocks of Language\\nLet’s first introduce what these blocks of language are to give context for the chal‐\\nlenges involved in NLP .\\nPhonemes\\nPhonemes are the smallest units of sound in a language. They may not have any\\nmeaning by themselves but can induce meanings when uttered in combination with\\nother phonemes. For example, standard English has 44 phonemes, which are either\\nsingle letters or a combination of letters [ 2]. Figure 1-4  shows these phonemes along\\nwith sample words. Phonemes are particularly important in applications involving\\nspeech understanding, such as speech recognition, speech-to-text transcription, and\\ntext-to-speech conversion.\\nWhat Is Language? | 9 Figure 1-4. Phonemes and examples\\nMorphemes and lexemes\\nA morpheme is the smallest unit of language that has a meaning. It is formed by a\\ncombination of phonemes. Not all morphemes are words, but all prefixes and suffixes\\nare morphemes. For example, in the word “multimedia, ” “multi-” is not a word but a\\nprefix that changes the meaning when put together with “media. ” “Multi-” is a mor‐\\npheme. Figure 1-5  illustrates some words and their morphemes. For words like “cats”\\nand “unbreakable, ” their morphemes are just constituents of the full word, whereas\\nfor words like “tumbling” and “unreliability, ” there is some variation when breaking\\nthe words down into their morphemes.\\nFigure 1-5. Morpheme examples\\nLexemes are the structural variations of morphemes related to one another by mean‐\\ning. For example, “run” and “running” belong to the same lexeme form. Morphologi‐\\ncal analysis, which analyzes the structure of words by studying its morphemes and\\nlexemes , is a foundational block for many NLP tasks, such as tokenization, stemming,\\n10 | Chapter 1: NLP: A Primer learning word embeddings, and part-of-speech tagging, which we’ll introduce in the\\nnext chapter.\\nSyntax\\nSyntax is a set of rules to construct grammatically correct sentences out of words and\\nphrases in a language. Syntactic structure in linguistics is represented in many differ‐\\nent ways. A common approach to representing sentences is a parse tree . Figure 1-6\\nshows an example parse tree for two English sentences.\\nFigure 1-6. Syntactic structure of two syntactically similar sentences\\nThis has a hierarchical structure of language, with words at the lowest level, followed\\nby part-of-speech tags, followed by phrases, and ending with a sentence at the highest\\nlevel. In Figure 1-6 , both sentences have a similar structure and hence a similar syn‐\\ntactic parse tree. In this representation, N stands for noun, V for verb, and P for prep‐\\nosition. Noun phrase is denoted by NP and verb phrase by VP . The two noun phrases\\nare “The girl” and “The boat, ” while the two verb phrases are “laughed at the monkey”\\nand “sailed up the river. ” The syntactic structure is guided by a set of grammar rules\\nfor the language (e.g., the sentence comprises an NP and a VP), and this in turn\\nguides some of the fundamental tasks of language processing, such as parsing. Pars‐\\ning is the NLP task of constructing such trees automatically. Entity extraction and\\nrelation extraction are some of the NLP tasks that build on this knowledge of parsing,\\nwhich we’ll discuss in more detail in Chapter 5 . Note that the parse structure\\ndescribed above is specific to English. The syntax of one language can be very differ‐\\nent from that of another language, and the language-processing approaches needed\\nfor that language will change accordingly.\\nWhat Is Language? | 11 Context\\nContext  is how various parts in a language come together to convey a particular\\nmeaning. Context includes long-term references, world knowledge, and common\\nsense along with the literal meaning of words and phrases. The meaning of a sentence\\ncan change based on the context, as words and phrases can sometimes have multiple\\nmeanings. Generally, context is composed from semantics and pragmatics. Semantics\\nis the direct meaning of the words and sentences without external context. Pragmat‐\\nics adds world knowledge and external context of the conversation to enable us to\\ninfer implied meaning. Complex NLP tasks such as sarcasm detection, summariza‐\\ntion, and topic modeling are some of tasks that use context heavily.\\nLinguistics is the study of language and hence is a vast area in itself, and we only\\nintroduced some basic ideas to illustrate the role of linguistic knowledge in NLP . Dif‐\\nferent tasks in NLP require varying degrees of knowledge about these building blocks\\nof language. An interested reader can refer to the books written by Emily Bender [ 3,\\n4] on the linguistic fundamentals for NLP for further study. Now that we have some\\nidea of what the building blocks of language are, let’s see why language can be hard\\nfor computers to understand and what makes NLP challenging.\\nWhy Is NLP Challenging?\\nWhat makes NLP a challenging problem domain? The ambiguity and creativity of\\nhuman language are just two of the characteristics that make NLP a demanding area\\nto work in. This section explores each characteristic in more detail, starting with\\nambiguity of language.\\nAmbiguity\\nAmbiguity means uncertainty of meaning. Most human languages are inherently\\nambiguous. Consider the following sentence: “I made her duck. ” This sentence has\\nmultiple meanings. The first one is: I cooked a duck for her. The second meaning is: I\\nmade her bend down to avoid an object. (There are other possible meanings, too;\\nwe’ll leave them for the reader to think of.) Here, the ambiguity comes from the use\\nof the word “made. ” Which of the two meanings applies depends on the context in\\nwhich the sentence appears. If the sentence appears in a story about a mother and a\\nchild, then the first meaning will probably apply. But if the sentence appears in a book\\nabout sports, then the second meaning will likely apply. The example we saw is a\\ndirect sentence.\\nWhen it comes to figurative language—i.e., idioms—the ambiguity only increases.\\nFor example, “He is as good as John Doe. ” Try to answer, “How good is he?” The\\nanswer depends on how good John Doe is. Figure 1-7  shows some examples illustrat‐\\ning ambiguity in language.\\n12 | Chapter 1: NLP: A Primer Figure 1-7. Examples of ambiguity in language from the Winograd Schema Challenge\\nThe examples come from the Winograd Schema Challenge  [5], named after Professor\\nTerry Winograd of Stanford University. This schema has pairs of sentences that differ\\nby only a few words, but the meaning of the sentences is often flipped because of this\\nminor change. These examples are easily disambiguated by a human but are not solv‐\\nable using most NLP techniques. Consider the pairs of sentences in the figure and the\\nquestions associated with them. With some thought, how the answer changes should\\nbe apparent based on a single word variation. As another experiment, consider taking\\nan off-the-shelf NLP system like Google Translate and try various examples to see\\nhow such ambiguities affect (or don’t affect) the output of the system.\\nCommon knowledge\\nA key aspect of any human language is “common knowledge. ” It is the set of all facts\\nthat most humans are aware of. In any conversation, it is assumed that these facts are\\nknown, hence they’re not explicitly mentioned, but they do have a bearing on the\\nmeaning of the sentence. For example, consider two sentences: “man bit dog” and\\n“dog bit man. ” We all know that the first sentence is unlikely to happen, while the sec‐\\nond one is very possible. Why do we say so? Because we all “know” that it is very\\nunlikely that a human will bite a dog. Further, dogs are known to bite humans. This\\nknowledge is required for us to say that the first sentence is unlikely to happen while\\nthe second one is possible. Note that this common knowledge was not mentioned in\\nWhat Is Language? | 13 either sentence. Humans use common knowledge all the time to understand and\\nprocess  any language. In the above example, the two sentences are syntactically very\\nsimilar, but a computer would find it very difficult to differentiate between the two, as\\nit lacks the common knowledge humans have. One of the key challenges in NLP is\\nhow to encode all the things that are common knowledge to humans in a computa‐\\ntional model.\\nCreativity\\nLanguage i'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_document[:80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,k in enumerate(docs):\n",
    "    print(f\"{index}: {k.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts,all_metadatas = [], []\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    # print(docs[i].page_content)\n",
    "    all_texts.append(docs[i].page_content)\n",
    "    all_metadatas.append({\"source\": f\"Document_{docs[i].metadata['page']}\"})\n",
    "    # print(docs[i].metadata['page'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sowmya Vajjala,  \n",
      "Bodhisattwa Majumder,  \n",
      "Anuj Gupta & Harshit SuranaPractical  \n",
      "Natural Language \n",
      "Processing\n",
      "A Comprehensive Guide to Building  \n",
      "Real-World NLP Systems\n",
      "{'source': 'Document_0'}\n"
     ]
    }
   ],
   "source": [
    "print(all_texts[0])\n",
    "print(all_metadatas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cohere Embeddings\n",
    "# COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "# cohere_embeddings = CohereEmbeddings(\n",
    "#     model=\"embed-english-v2.0\",\n",
    "#     cohere_api_key=COHERE_API_KEY)\n",
    "\n",
    "\n",
    "model=\"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "embeddings = HuggingFaceEndpointEmbeddings(model=model)\n",
    "\n",
    "# Create Deep Lake dataset\n",
    "ACTIVELOOP_TOKEN = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "my_activeloop_org_id = \"danllm\"\n",
    "my_activeloop_dataset_name = \"info_grabber\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding=embeddings,overwrite=True)\n",
    "\n",
    "# add documents to out Deep lake dataset\n",
    "db.add_texts(all_texts,all_metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RetrievalQAWithSourcesChain WITHOUT A RERANKER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_kwargs = {\"k\":4}\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    \"question\": \"what is NLP?\"\n",
    "}\n",
    "d_response = qa.invoke(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is NLP?\n",
      "\n",
      "Source(s):\n"
     ]
    }
   ],
   "source": [
    "print(d_response['question'])\n",
    "print(d_response['answer'])\n",
    "print(\"Source(s):\")\n",
    "for source in d_response['sources'].split():\n",
    "    print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [{\"question\": \"what is NLP?\"}, {\"question\": \"what are the pipelines of NLP?\"}]\n",
    "# d_response = qa.invoke(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is NLP?\n",
      "\n",
      "Source(s):\n",
      "what are the pipelines of NLP?\n",
      "\n",
      "Source(s):\n"
     ]
    }
   ],
   "source": [
    "for question in input_data:\n",
    "    d_response = qa.invoke(question)\n",
    "    print(d_response['question'])\n",
    "    print(d_response['answer'])\n",
    "    print(\"Source(s):\")\n",
    "    for source in d_response['sources'].split():\n",
    "        print(f\"- {source}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **WITH A RERANKER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = langchain_cohere_reranker(model='rerank-english-v3.0')\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\"what are the pipelines of NLP?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We’ll start the discussion by revisiting the NLP pipeline we introduced in Chapter 2\n",
      "and take a look at the last two steps: deployment, followed by monitoring and updat‐\n",
      "ing the model, which we didn’t cover in earlier chapters. We’ll also see what it takes to\n",
      "build and maintain a mature NLP system. This is followed by a discussion on the data\n",
      "science processes followed in various AI teams, especially with respect to building\n",
      "NLP software in particular. We’ll conclude the chapter with a lot of recommenda‐\n",
      "tions, best practices, and do’s and don’ts to successfully deliver NLP projects. Let’s\n",
      "start by looking at how to deploy NLP software.\n",
      "Revisiting the NLP Pipeline: Deploying NLP Software\n",
      "In Chapter 2 , we saw that a typical production pipeline for NLP projects consists of\n",
      "the following stages: data acquisition, text cleaning, text pre-processing, text repre‐\n",
      "sentation and feature engineering, modeling, evaluation, deployment, monitoring,\n",
      "and model updating. When we encounter a new problem scenario involving NLP in\n",
      "our organization, we have to first start thinking about creating an NLP pipeline cov‐\n",
      "ering these stages. Some of the questions we should ask ourselves in this process are:\n",
      "•What kind of data do we need for training the NLP system? Where do we get this\n",
      "data from? These questions are important at the start and also later as the model\n",
      "matures.\n",
      "•How much data is available? If it’s not enough, what data augmentation techni‐\n",
      "ques can we try?\n",
      "•How will we label the data, if necessary?\n",
      "•How will we quantify the performance of our model? What metrics will we use to\n",
      "do that?\n",
      "•How will we deploy the system? Using API calls over the cloud, or a monolith\n",
      "system, or an embedded module on an edge device?\n",
      "•How will the predictions be served: streaming or batch process?\n",
      "•Would we need to update the model? If yes, what will the update frequency be:\n",
      "daily, weekly, monthly?\n",
      "•Do we need a monitoring and alerting mechanism for model performance? If\n"
     ]
    }
   ],
   "source": [
    "print(compressed_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      "problem down into several sub-problems , then try to develop a step-by-step \n",
      "procedure  to solve them. Since language  processing  is involved , we would also\n",
      "list all the forms of text processing  needed at each step. This step-by-step \n",
      "processing  of text is known as pipeline . It is the series of steps involved  in\n",
      "building  any NLP model. These steps are common in every NLP project, so it \n",
      "makes sense to study them in this chapter. Understanding  some common procedures\n",
      "in any NLP pipeline  will enable us to get started on any NLP problem encountered  \n",
      "in the workplace . Laying out and developing  a text-processing  pipeline  is seen \n",
      "as a starting  point for any NLP application  development  process. In this\n",
      "chapter, we will learn about the various steps involved  and how they play  \n",
      "important  roles in solving the NLP problem and we’ll see a few guidelines\n",
      "about when and how to use which step. In later chapters , we’ll discuss  \n",
      "specific  pipelines  for various NLP tasks (e.g., Chapters  4–7).\"\n",
      "my_sentences  = sent_tokenize (mytext)\n",
      "50 | Chapter 2: NLP Pipeline\n",
      "\n",
      "We’ll start the discussion by revisiting the NLP pipeline we introduced in Chapter 2\n",
      "and take a look at the last two steps: deployment, followed by monitoring and updat‐\n",
      "ing the model, which we didn’t cover in earlier chapters. We’ll also see what it takes to\n",
      "build and maintain a mature NLP system. This is followed by a discussion on the data\n",
      "science processes followed in various AI teams, especially with respect to building\n",
      "NLP software in particular. We’ll conclude the chapter with a lot of recommenda‐\n",
      "tions, best practices, and do’s and don’ts to successfully deliver NLP projects. Let’s\n",
      "start by looking at how to deploy NLP software.\n",
      "Revisiting the NLP Pipeline: Deploying NLP Software\n",
      "In Chapter 2 , we saw that a typical production pipeline for NLP projects consists of\n",
      "the following stages: data acquisition, text cleaning, text pre-processing, text repre‐\n",
      "sentation and feature engineering, modeling, evaluation, deployment, monitoring,\n",
      "and model updating. When we encounter a new problem scenario involving NLP in\n",
      "our organization, we have to first start thinking about creating an NLP pipeline cov‐\n",
      "ering these stages. Some of the questions we should ask ourselves in this process are:\n",
      "•What kind of data do we need for training the NLP system? Where do we get this\n",
      "data from? These questions are important at the start and also later as the model\n",
      "matures.\n",
      "•How much data is available? If it’s not enough, what data augmentation techni‐\n",
      "ques can we try?\n",
      "•How will we label the data, if necessary?\n",
      "•How will we quantify the performance of our model? What metrics will we use to\n",
      "do that?\n",
      "•How will we deploy the system? Using API calls over the cloud, or a monolith\n",
      "system, or an embedded module on an edge device?\n",
      "•How will the predictions be served: streaming or batch process?\n",
      "•Would we need to update the model? If yes, what will the update frequency be:\n",
      "daily, weekly, monthly?\n",
      "•Do we need a monitoring and alerting mechanism for model performance? If\n",
      "\n",
      "We also saw how a traditional NLP pipeline and a DL-based NLP pipeline differ from\n",
      "each other and learned what to do when working with non-English languages. Aside\n",
      "from the case study, we looked at these steps in a more general manner in this chap‐\n",
      "ter. Specific details for each step will depend on the task at hand and the purpose of\n",
      "our implementation. We’ll look at a few task-specific pipelines from Chapter 4\n",
      "onward, describing in detail what’s unique as well as common across different tasks\n",
      "while designing such pipelines. In the next chapter, we’ll tackle the question of text\n",
      "representation that we mentioned briefly earlier in this chapter.\n",
      "References\n",
      "[1] Iderhoff, Nicolas. nlp-datasets: Alphabetical list of free/public domain datasets\n",
      "with text data for use in Natural Language Processing (NLP) , (GitHub repo). Last\n",
      "accessed June 15, 2020.\n",
      "[2] Google. “Dataset Search” . Last accessed June 15, 2020.\n",
      "[3] Miller, George A. “WordNet: A Lexical Database for English. ” Communications of\n",
      "the ACM  38.11 (1995): 39–41.\n",
      "[4] NTLTK documentation. “WordNet Interface” . Last accessed June 15, 2020.\n",
      "76 | Chapter 2: NLP Pipeline\n",
      "Source(s):\n",
      "- Document_79\n",
      "- Document_401\n",
      "- Document_105\n"
     ]
    }
   ],
   "source": [
    "retrieved_chunks = [doc.page_content for doc in compressed_docs]\n",
    "\n",
    "# format prompt\n",
    "final_answer = \"\\n\\n\".join(retrieved_chunks).strip()\n",
    "print(f\"Final Answer:\\n{final_answer}\")\n",
    "try:\n",
    "    sources = []\n",
    "    for i in range(len(compressed_docs)):\n",
    "        source = compressed_docs[i].metadata['source']\n",
    "        if not any(source == s for s in sources):\n",
    "            sources.append(source)\n",
    "except IndexError as error:\n",
    "    print(\"No sources\")\n",
    "    sources.append(\"No sources\")\n",
    "\n",
    "print(\"Source(s):\")\n",
    "for source in sources:\n",
    "    print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using the reranker retriever with a QA pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is Natural Language Processing?',\n",
       " 'result': ' Natural Language Processing (NLP) is a field at the intersection of computer'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=compression_retriever\n",
    ")\n",
    "\n",
    "input_data = {\n",
    "    \"query\": \"what is Natural Language Processing?\"\n",
    "}\n",
    "qa.invoke(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RetrievalQAWithSourcesChain wih reranker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = compression_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    # \"question\": \"what is NLP?\",\n",
    "    \"question\": \"what are the pipelines of NLP?\"\n",
    "}\n",
    "d_response = qa.invoke(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what are the pipelines of NLP?', 'answer': '', 'sources': ''}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the pipelines of NLP?\n",
      "\n",
      "Source(s):\n"
     ]
    }
   ],
   "source": [
    "print(d_response['question'])\n",
    "print(d_response['answer'])\n",
    "print(\"Source(s):\")\n",
    "for source in d_response['sources'].split():\n",
    "    print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using Similarity Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      "one-hot  encoding scheme cannot handle this. The only way is to retrain the\n",
      "model: start by expanding the vocabulary, give an ID to the new word, etc.\n",
      "These days, one-hot encoding scheme is seldom used.\n",
      "Some of these shortcomings can be addressed by the bag-of-words approach\n",
      "described next.\n",
      "Bag of Words\n",
      "Bag of words (BoW) is a classical text representation technique that has been used\n",
      "commonly in NLP , especially in text classification problems (see Chapter 4 ). The key\n",
      "idea behind it is as follows: represent the text under consideration as a bag (collec‐\n",
      "tion) of words  while ignoring the order and context. The basic intuition behind it is\n",
      "that it assumes that the text belonging to a given class in the dataset is characterized\n",
      "by a unique set of words. If two text pieces have nearly the same words, then they\n",
      "belong to the same bag (class). Thus, by analyzing the words present in a piece of text,\n",
      "one can identify the class (bag) it belongs to.\n",
      "Similar to one-hot encoding, BoW maps words to unique integer IDs between 1 and\n",
      "|V|. Each document in the corpus is then converted into a vector of |V| dimensions\n",
      "where in the ith component of the vector, i = wid, is simply the number of times the\n",
      "word w occurs in the document, i.e., we simply score each word in V by their occur‐\n",
      "rence count in the document.\n",
      "Thus, for our toy corpus ( Table 3-1 ), where the word IDs are dog = 1, bites = 2, man\n",
      "= 3, meat = 4 , food = 5, eats = 6, D1 becomes [1 1 1 0 0 0]. This is because the first\n",
      "three words in the vocabulary appeared exactly once in D1, and the last three did not\n",
      "appear at all. D4 becomes [0 0 1 0 1 1]. The notebook Ch3/Bag_of_Words.ipynb  dem‐\n",
      "onstrates how we can implement BoW text representation. The following code shows\n",
      "the key parts:\n",
      "from sklearn.feature_extraction.text  import CountVectorizer\n",
      "count_vect  = CountVectorizer ()\n",
      "#Build a BOW representation for the corpus\n",
      "bow_rep = count_vect .fit_transform (processed_docs )\n",
      "#Look at the vocabulary mapping\n",
      "\n",
      "Let’s look at a simple way to implement this in Python from first principles. The note‐\n",
      "book Ch3/OneHotEncoding.ipynb  demonstrates an example of this. The code that fol‐\n",
      "lows is borrowed from the notebook and implements one-hot encoding. In real-\n",
      "world projects, we mostly use scikit-learn’s implementation of one-hot encoding,\n",
      "which is much more optimized. We’ve provided the same in the notebook.\n",
      "Since we assume that the text is tokenized, we can just split the text on white space in\n",
      "this example:\n",
      "def get_onehot_vector (somestring ):\n",
      "  onehot_encoded  = []\n",
      "  for word in somestring .split():\n",
      "             temp = [0]*len(vocab)\n",
      "             if word in vocab:\n",
      "                        temp[vocab[word]-1] = 1\n",
      "             onehot_encoded .append(temp)\n",
      "  return onehot_encoded\n",
      "get_onehot_vector (processed_docs [1])\n",
      "Output: [[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]\n",
      "Now that we understand the scheme, let’s discuss some of its pros and cons. On the\n",
      "positive side, one-hot encoding is intuitive to understand and straightforward to\n",
      "implement. However, it suffers from a few shortcomings:\n",
      "•The size of a one-hot vector is directly proportional to size of the vocabulary, and\n",
      "most real-world corpora have large vocabularies. This results in a sparse repre‐\n",
      "sentation where most of the entries in the vectors are zeroes, making it computa‐\n",
      "tionally inefficient to store, compute with, and learn from (sparsity leads to\n",
      "overfitting).\n",
      "•This representation does not give a fixed-length representation for text, i.e., if a\n",
      "text has 10 words, you get a longer representation for it as compared to a text\n",
      "with 5 words. For most learning algorithms, we need the feature vectors to be of\n",
      "the same length.\n",
      "•It treats words as atomic units and has no notion of (dis)similarity between\n",
      "words. For example, consider three words: run, ran, and apple. Run and ran have\n",
      "similar meanings as opposed to run and apple. But if we take their respective vec‐\n",
      "\n",
      "Building THE Model                                                                                                 65\n",
      "Evaluation                                                                                                                       68\n",
      "Intrinsic Evaluation                                                                                                    68\n",
      "Extrinsic Evaluation                                                                                                   71\n",
      "Post-Modeling Phases                                                                                                   72\n",
      "Deployment                                                                                                                 72\n",
      "Monitoring                                                                                                                  72\n",
      "Model Updating                                                                                                          73\n",
      "Working with Other Languages                                                                                   73\n",
      "Case Study                                                                                                                       74\n",
      "Wrapping Up                                                                                                                  76\n",
      "3.Text Representation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  81\n",
      "Vector Space Models                                                                                                     84\n",
      "Basic Vectorization Approaches                                                                                  85\n",
      "One-Hot Encoding                                                                                                     85\n",
      "Bag of Words                                                                                                               87\n",
      "Source(s):\n",
      "- Document_116\n",
      "- Document_115\n",
      "- Document_9\n"
     ]
    }
   ],
   "source": [
    "# query = \"what are the pipelines of NLP?\"\n",
    "query = \"what is one-hot encoding?\"\n",
    "docs = db.similarity_search(query=query,k=3)\n",
    "retrieved_chunks = [doc.page_content for doc in docs]\n",
    "# format prompt\n",
    "final_answer = \"\\n\\n\".join(retrieved_chunks).strip()\n",
    "print(f\"Final Answer:\\n{final_answer}\")\n",
    "\n",
    "try:\n",
    "    sources = []\n",
    "    for i in range(len(docs)):\n",
    "        source = docs[i].metadata['source']\n",
    "        if not any(source == s for s in sources):\n",
    "            sources.append(source)\n",
    "except IndexError as error:\n",
    "    print(\"No sources\")\n",
    "    sources.append(\"No sources\")\n",
    "\n",
    "print(\"Source(s):\")\n",
    "for source in sources:\n",
    "    print(f\"- {source}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
