{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace\n",
    "# from langchain.llms import HuggingFaceHub\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "# Prompts\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt,\\\n",
    "    PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import HumanMessage, SystemMessage,AIMessage\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector,\\\n",
    "    SemanticSimilarityExampleSelector\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "# Output parsing\n",
    "from langchain.output_parsers import PydanticOutputParser,\\\n",
    "    OutputFixingParser,CommaSeparatedListOutputParser,\\\n",
    "        RetryWithErrorOutputParser,\\\n",
    "        StructuredOutputParser,ResponseSchema\n",
    "# from pydantic import BaseModel,Field,field_validator\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import List,Dict\n",
    "# Scapers,Retrievers,Loaders,Splitters,Embeddings\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.chains import RetrievalQA,LLMChain,\\\n",
    "    ConversationalRetrievalChain, ConversationChain,\\\n",
    "        SimpleSequentialChain,RetrievalQAWithSourcesChain\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,\\\n",
    "    CharacterTextSplitter,NLTKTextSplitter,SpacyTextSplitter,\\\n",
    "    MarkdownTextSplitter,TokenTextSplitter,Language\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.document_loaders import PyPDFLoader, SeleniumURLLoader,\\\n",
    "    GoogleDriveLoader\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor,\\\n",
    "    LLMChainFilter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_cohere import CohereEmbeddings, \\\n",
    "    CohereRerank as langchain_cohere_reranker\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "# ConstitutionalAI\n",
    "from langchain.chains.constitutional_ai.base import ConstitutionalChain\n",
    "from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\n",
    "# Conversation, Chat & Memory\n",
    "from langchain.prompts.chat import ChatPromptTemplate,\\\n",
    "    SystemMessagePromptTemplate,\\\n",
    "        AIMessagePromptTemplate,HumanMessagePromptTemplate,\\\n",
    "        MessagesPlaceholder\n",
    "from langchain import ConversationChain\n",
    "from  langchain.memory import ConversationBufferMemory,\\\n",
    "    ConversationSummaryBufferMemory, ConversationSummaryMemory,\\\n",
    "    ConversationBufferWindowMemory\n",
    "# Agents and Tools\n",
    "from langchain.agents import  tool # this is a decorator\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentType,initialize_agent,\\\n",
    "    load_tools, Tool,\\\n",
    "        AgentExecutor,\\\n",
    "        create_json_agent,create_react_agent,create_structured_chat_agent\n",
    "from langchain_google_community import GoogleSearchAPIWrapper\n",
    "from langchain.tools import DuckDuckGoSearchRun,DuckDuckGoSearchResults\n",
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain_experimental.autonomous_agents import BabyAGI, AutoGPT\n",
    "from langchain.tools.file_management import WriteFileTool,ReadFileTool\n",
    "from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
    "import faiss\n",
    "# LlamaIndex\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import download_loader\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.core.service_context import ServiceContext\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine,\\\n",
    "    SubQuestionQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.core.indices import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool,ToolMetadata\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.evaluation import generate_question_context_pairs,\\\n",
    "    EmbeddingQAFinetuneDataset,RelevancyEvaluator,\\\n",
    "    FaithfulnessEvaluator,BatchEvalRunner,RetrieverEvaluator\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.instructor import InstructorEmbedding\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank as llama_index_cohere_reranker\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank,LLMRerank\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI as llama_hugging_face_api\n",
    "from llama_index.llms.text_generation_inference import TextGenerationInference\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.readers.github import GithubRepositoryReader,GithubClient\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings as langchain_embeddings_hugging_face_api\n",
    "# from llama_index.llms.langchain import LangChainLLM # causing conflicts\n",
    "# DeepLake\n",
    "import deeplake\n",
    "# Others\n",
    "from newspaper import Article\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "from elevenlabs.client import ElevenLabs\n",
    "from streamlit_chat import message\n",
    "from audio_recorder_streamlit import audio_recorder\n",
    "import streamlit as st\n",
    "import cohere\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import seaborn as sns\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import yt_dlp\n",
    "import whisper\n",
    "import textwrap\n",
    "import re\n",
    "import tiktoken\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Provide the filename as a string\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# repo_id = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
    "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Requires Pro suscription\n",
    "# repo_id = \"meta-llama/Meta-Llama-3-70B-Instruct\" \n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_new_tokens\": 1000, # Maximum tokens to generate\n",
    "    \"max_length\": 4000, # Maximum length of input + output\n",
    "    \"temperature\": 0.1, # Controls randomness of output\n",
    "    \"timeout\": 6000,\n",
    "    # \"task\":'conversational',\n",
    "}\n",
    "# Input validation error: `inputs` tokens + `max_new_tokens` \n",
    "# must be <= 8192. Given: 2902 `inputs` tokens and 6000 `max_new_tokens`\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN,\n",
    "    # you specify the task or not\n",
    "    # You can also specify the task in the model_kwargs or within here\n",
    "    # task = 'conversational',\n",
    "    **model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cohere Embeddings\n",
    "# COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "# cohere_embeddings = CohereEmbeddings(\n",
    "#     model=\"embed-english-v2.0\",\n",
    "#     cohere_api_key=COHERE_API_KEY)\n",
    "\n",
    "\n",
    "model=\"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "embeddings = HuggingFaceEndpointEmbeddings(model=model)\n",
    "\n",
    "# Create Deep Lake dataset\n",
    "ACTIVELOOP_TOKEN = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "my_activeloop_org_id = \"danllm\"\n",
    "my_activeloop_dataset_name = \"info_grabber\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding=embeddings,read_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_summariser = \"\"\"\n",
    "As an NLP specialist, summarise the given text below in 5 bulletins\n",
    "Text: {text}\n",
    "Summary:\"\"\"\n",
    "prompt_summariser_input_variables = ['text']\n",
    "prompt_summariser = PromptTemplate(\n",
    "    template=prompt_template_summariser,\n",
    "    input_variables=prompt_summariser_input_variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RetrievalQAWithSourcesChain WITHOUT A RERANKER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "input_data = {'question':\"what is Bag of Words?\"}\n",
    "search_kwargs = {\"k\":4}\n",
    "retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = retriever\n",
    ")\n",
    "d_response = qa.invoke(input_data)\n",
    "print(f\"{d_response['question']}\\n\")\n",
    "print(f\"Final Answer:\\n{d_response['answer']}\")\n",
    "# Extract the answer\n",
    "summariser_input_data = {\"text\":d_response['answer'].strip()}\n",
    "llm_summariser_chain = prompt_summariser | llm\n",
    "# Summarise the answer\n",
    "summariser_response = llm_summariser_chain.invoke(summariser_input_data)\n",
    "print(f\"Summarised answer:\\n{summariser_response}\")\n",
    "print(\"Source(s):\")\n",
    "for source in d_response['sources'].split():\n",
    "    if source == \"\":\n",
    "        print(\"No sources\")\n",
    "    else:\n",
    "        print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **WITH A RERANKER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = langchain_cohere_reranker(model='rerank-english-v3.0')\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\"what are the pipelines of NLP?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compressed_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_chunks = [doc.page_content for doc in compressed_docs]\n",
    "# format prompt\n",
    "final_answer = \"\\n\\n\".join(retrieved_chunks).strip()\n",
    "# print(f\"Final Answer:\\n{final_answer}\")\n",
    "# Extract the answer\n",
    "summariser_input_data = {\"text\":final_answer.strip()}\n",
    "llm_summariser_chain = prompt_summariser | llm\n",
    "# Summarise the answer\n",
    "summariser_response = llm_summariser_chain.invoke(summariser_input_data)\n",
    "print(f\"Summarised answer:\\n{summariser_response}\")\n",
    "try:\n",
    "    sources = []\n",
    "    for i in range(len(compressed_docs)):\n",
    "        source = compressed_docs[i].metadata['source']\n",
    "        if not any(source == s for s in sources):\n",
    "            sources.append(source)\n",
    "except IndexError as error:\n",
    "    print(\"No sources\")\n",
    "    sources.append(\"No sources\")\n",
    "\n",
    "print(\"Source(s):\")\n",
    "for source in sources:\n",
    "    print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using the reranker retriever with a QA pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is Bag of Words? ',\n",
       " 'result': ' Bag of Words is a simple and intuitive way to represent text data. It is a type of feature extraction technique that represents a document as a bag, or a set, of its word frequencies. In other words, it is a vector where each dimension corresponds to a unique word in the vocabulary, and the value at that dimension is the frequency of that word in the document. For example, if we have a document with the words “the”, “quick”, “brown”, “fox”, and “jumps” and their frequencies are 2, 1, 1, 1, and 1 respectively, then the Bag of Words representation of this document would be [2, 1, 1, 1, 1]. This representation is often used as input to machine learning algorithms. It is a simple and effective way to capture the semantic meaning of a document.'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=compression_retriever\n",
    ")\n",
    "\n",
    "input_data = {\n",
    "    \"query\": \"what is Bag of Words? \"\n",
    "}\n",
    "qa.invoke(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is Bag of Words? \n",
      "\n",
      "Final Answer:\n",
      " Bag of Words is a simple and intuitive way to represent text data. It is a type of feature extraction technique that represents a document as a bag, or a set, of its word frequencies. In other words, it is a vector where each dimension corresponds to a unique word in the vocabulary, and the value at that dimension is the frequency of that word in the document. For example, if we have a document with the words “the”, “quick”, “brown”, “fox”, and “jumps” and their frequencies are 2, 1, 1, 1, and 1 respectively, then the Bag of Words representation of this document would be [2, 1, 1, 1, 1]. This representation is often used as input to machine learning algorithms. It is a simple and effective way to capture the semantic meaning of a document.\n",
      "Summarised answer:\n",
      " \n",
      "• Bag of Words is a feature extraction technique that represents a document as a set of word frequencies.\n",
      "• Each dimension in the vector corresponds to a unique word in the vocabulary, and the value at that dimension is the frequency of that word in the document.\n",
      "• The Bag of Words representation is often used as input to machine learning algorithms.\n",
      "• It is a simple and effective way to capture the semantic meaning of a document.\n",
      "• The representation is a vector where each dimension corresponds to a unique word in the vocabulary, and the value at that dimension is the frequency of that word in the document.\n"
     ]
    }
   ],
   "source": [
    "d_response = qa.invoke(input_data)\n",
    "print(f\"{d_response['query']}\\n\")\n",
    "print(f\"Final Answer:\\n{d_response['result']}\")\n",
    "# Extract the answer\n",
    "summariser_input_data = {\"text\":d_response['result'].strip()}\n",
    "llm_summariser_chain = prompt_summariser | llm\n",
    "# Summarise the answer\n",
    "summariser_response = llm_summariser_chain.invoke(summariser_input_data)\n",
    "print(f\"Summarised answer:\\n{summariser_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RetrievalQAWithSourcesChain wih reranker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = compression_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the steps for NLP?\n",
      "\n",
      "Final Answer:\n",
      " 380\n",
      ".....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Summarised answer:\n",
      " \n",
      "• The text is a collection of random words and phrases, with no clear meaning or structure.\n",
      "• The text appears to be a jumbled mess of words, with no discernible pattern or organization.\n",
      "• The text does not contain any coherent sentences or paragraphs, making it difficult to understand or summarize.\n",
      "• The text may be a result of a random word generator or a typo-filled document.\n",
      "• The text is not suitable for analysis or summarization due to its lack of meaning and structure......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Summary: \n",
      "• The text is a collection of random words and phrases, with no clear meaning or structure.\n",
      "• The text appears to be a jumbled mess of words, with no discernible pattern or organization.\n",
      "• The text does not contain any coherent sentences or paragraphs, making it difficult to understand or summarize.\n",
      "• The text may be a result of a random word generator or a typo-filled document.\n",
      "• The text is not suitable for analysis or summarization due to its lack of meaning and structure......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Summary: \n",
      "• The text is a collection of random words and phrases, with no clear meaning or structure.\n",
      "• The text appears to be a jumbled mess of words, with no discernible pattern or organization.\n",
      "• The text does not contain any coherent sentences or paragraphs, making it difficult to understand or summarize.\n",
      "• The text may be a result of a random word generator or a typo-filled document.\n",
      "• The text is not suitable for analysis or summarization due to its lack of meaning and structure......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Summary: \n",
      "• The text is a collection of random words and phrases, with no clear meaning or structure.\n",
      "• The text appears to be a jumbled mess of words, with no discernible pattern or organization.\n",
      "• The text does not contain any coherent sentences or paragraphs, making it difficult to understand or summarize.\n",
      "• The text may be a result of a random word generator or a typo-filled document.\n",
      "• The text is not suitable for analysis or summarization due to its lack of meaning and structure......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Summary: \n",
      "• The text is a collection of random words and phrases, with no clear meaning or structure.\n",
      "• The text appears to be a jumbled mess of words, with no discernible pattern or organization.\n",
      "• The text does not contain any coherent sentences or paragraphs, making it difficult to understand or summarize.\n",
      "• The text may be a result of a random word generator or a typo-filled document.\n",
      "• The text is not suitable for analysis or summarization due to its lack of meaning and structure......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Summary: \n",
      "• The text is a collection of random words and phrases, with no clear meaning or structure.\n",
      "• The text appears to be a jumbled mess of words, with no discernible pattern or organization.\n",
      "• The text does not contain any coherent sentences or paragraphs, making it difficult to understand or summarize.\n",
      "• The text may be a result of a random word generator or a typo-filled document.\n",
      "• The text is not suitable for analysis or summarization due to its lack of meaning and structure......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Summary: \n",
      "• The text is a collection of random words and phrases, with no clear meaning or structure.\n",
      "• The text appears to be a jumbled mess of words, with no discernible pattern or organization.\n",
      "• The text does not contain any coherent sentences or paragraphs, making it difficult to understand or summarize.\n",
      "• The text may be a result of a random word generator or a typo-filled document.\n",
      "• The text is not suitable for analysis or summarization due to its lack of meaning and structure......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Summary: \n",
      "• The text is a collection of random words and phrases, with no clear meaning or structure.\n",
      "• The text appears to be a jumbled mess of words, with no discernible pattern or organization.\n",
      "• The text does not contain any coherent sentences or paragraphs, making it difficult to understand or summarize.\n",
      "• The text may be a result of a random word generator or a typo-filled document.\n",
      "• The text is not suitable for analysis or summarization due to its lack of meaning and structure......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Summary: \n",
      "• The text is a collection of random words and phrases, with no clear meaning or structure.\n",
      "• The text appears to be a jumbled mess of words, with no discernible pattern or organization.\n",
      "• The text does not contain any\n",
      "Source(s):\n"
     ]
    }
   ],
   "source": [
    "# Doesn't work well\n",
    "input_data = {'question':\"what are the steps for NLP?\"}\n",
    "d_response = qa.invoke(input_data)\n",
    "print(f\"{d_response['question']}\\n\")\n",
    "print(f\"Final Answer:\\n{d_response['answer']}\")\n",
    "# Extract the answer\n",
    "summariser_input_data = {\"text\":d_response['answer'].strip()}\n",
    "llm_summariser_chain = prompt_summariser | llm\n",
    "# Summarise the answer\n",
    "summariser_response = llm_summariser_chain.invoke(summariser_input_data)\n",
    "print(f\"Summarised answer:\\n{summariser_response}\")\n",
    "print(\"Source(s):\")\n",
    "for source in d_response['sources'].split():\n",
    "    if source == \"\":\n",
    "        print(\"No sources\")\n",
    "    else:\n",
    "        print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using Similarity Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the steps of NLP?\n",
      "\n",
      "Final Answer:\n",
      "problem down into several sub-problems , then try to develop a step-by-step \n",
      "procedure  to solve them. Since language  processing  is involved , we would also\n",
      "list all the forms of text processing  needed at each step. This step-by-step \n",
      "processing  of text is known as pipeline . It is the series of steps involved  in\n",
      "building  any NLP model. These steps are common in every NLP project, so it \n",
      "makes sense to study them in this chapter. Understanding  some common procedures\n",
      "in any NLP pipeline  will enable us to get started on any NLP problem encountered  \n",
      "in the workplace . Laying out and developing  a text-processing  pipeline  is seen \n",
      "as a starting  point for any NLP application  development  process. In this\n",
      "chapter, we will learn about the various steps involved  and how they play  \n",
      "important  roles in solving the NLP problem and we’ll see a few guidelines\n",
      "about when and how to use which step. In later chapters , we’ll discuss  \n",
      "specific  pipelines  for various NLP tasks (e.g., Chapters  4–7).\"\n",
      "my_sentences  = sent_tokenize (mytext)\n",
      "50 | Chapter 2: NLP Pipeline\n",
      "\n",
      "processing steps used in NLP software:\n",
      "Preliminaries\n",
      "Sentence segmentation and word tokenization.\n",
      "Frequent steps\n",
      "Stop word removal, stemming and lemmatization, removing digits/punctuation,\n",
      "lowercasing, etc.\n",
      "Other steps\n",
      "Normalization, language detection, code mixing, transliteration, etc.\n",
      "Advanced processing\n",
      "POS tagging, parsing, coreference resolution, etc.\n",
      "While not all steps will be followed in all the NLP pipelines we encounter, the first\n",
      "two are more or less seen everywhere. Let’s take a look at what each of these steps\n",
      "mean.\n",
      "Pre-Processing | 49\n",
      "\n",
      "Summarised answer:\n",
      " The NLP pipeline is a series of steps involved in building any NLP model. It is a common procedure in every NLP project. Understanding the common procedures in any NLP pipeline enables us to get started on any NLP problem encountered in the workplace. The pipeline includes various steps such as sentence segmentation, word tokenization, stop word removal, stemming and lemmatization, removing digits/punctuation, lowercasing, normalization, language detection, code mixing, transliteration, POS tagging, parsing, and coreference resolution.\n",
      "\n",
      "Here are the 5 bullet points summarizing the text:\n",
      "\n",
      "• The NLP pipeline is a series of steps involved in building any NLP model, and it is a common procedure in every NLP project.\n",
      "• The pipeline includes various steps such as sentence segmentation, word tokenization, stop word removal, stemming and lemmatization, removing digits/punctuation, lowercasing, normalization, language detection, code mixing, transliteration, POS tagging, parsing, and coreference resolution.\n",
      "• Understanding the common procedures in any NLP pipeline enables us to get started on any NLP problem encountered in the workplace.\n",
      "• The first two steps, sentence segmentation and word tokenization, are more or less seen everywhere in NLP pipelines.\n",
      "• The NLP pipeline is a starting point for any NLP application development process, and it is essential to study the various steps involved and how they play important roles in solving NLP problems.\n",
      "Source(s):\n",
      "- Document_79\n",
      "- Document_78\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the steps of NLP?\"\n",
    "\n",
    "docs = db.similarity_search(query=query, k=2)\n",
    "# retrieve relevant chunks\n",
    "retrieved_chunks = [doc.page_content for doc in docs]\n",
    "# format prompt\n",
    "final_answer_resource = \"\\n\\n\".join(retrieved_chunks).strip()\n",
    "\n",
    "print(f\"{query}\\n\")\n",
    "print(f\"Final Answer:\\n{final_answer_resource}\\n\")\n",
    "# Extract the answer\n",
    "summariser_input_data = {\"text\":final_answer_resource}\n",
    "llm_summariser_chain = prompt_summariser | llm\n",
    "summariser_response = llm_summariser_chain.invoke(summariser_input_data)\n",
    "print(f\"Summarised answer:\\n{summariser_response}\")\n",
    "try:\n",
    "    sources = []\n",
    "    for i in range(len(docs)):\n",
    "        source = docs[i].metadata['source']\n",
    "        if not any(source == s for s in sources):\n",
    "            sources.append(source)\n",
    "except IndexError as error:\n",
    "    print(\"No sources\")\n",
    "    sources.append(\"No sources\")\n",
    "\n",
    "print(\"Source(s):\")\n",
    "for source in sources:\n",
    "    print(f\"- {source}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
