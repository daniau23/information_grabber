{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace\n",
    "# from langchain.llms import HuggingFaceHub\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "# Prompts\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt,\\\n",
    "    PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import HumanMessage, SystemMessage,AIMessage\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector,\\\n",
    "    SemanticSimilarityExampleSelector\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "# Output parsing\n",
    "from langchain.output_parsers import PydanticOutputParser,\\\n",
    "    OutputFixingParser,CommaSeparatedListOutputParser,\\\n",
    "        RetryWithErrorOutputParser,\\\n",
    "        StructuredOutputParser,ResponseSchema\n",
    "# from pydantic import BaseModel,Field,field_validator\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import List,Dict\n",
    "# Scapers,Retrievers,Loaders,Splitters,Embeddings\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.chains import RetrievalQA,LLMChain,\\\n",
    "    ConversationalRetrievalChain, ConversationChain,\\\n",
    "        SimpleSequentialChain,RetrievalQAWithSourcesChain\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,\\\n",
    "    CharacterTextSplitter,NLTKTextSplitter,SpacyTextSplitter,\\\n",
    "    MarkdownTextSplitter,TokenTextSplitter,Language\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.document_loaders import PyPDFLoader, SeleniumURLLoader,\\\n",
    "    GoogleDriveLoader\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor,\\\n",
    "    LLMChainFilter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_cohere import CohereEmbeddings, \\\n",
    "    CohereRerank as langchain_cohere_reranker\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "# ConstitutionalAI\n",
    "from langchain.chains.constitutional_ai.base import ConstitutionalChain\n",
    "from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\n",
    "# Conversation, Chat & Memory\n",
    "from langchain.prompts.chat import ChatPromptTemplate,\\\n",
    "    SystemMessagePromptTemplate,\\\n",
    "        AIMessagePromptTemplate,HumanMessagePromptTemplate,\\\n",
    "        MessagesPlaceholder\n",
    "from langchain import ConversationChain\n",
    "from  langchain.memory import ConversationBufferMemory,\\\n",
    "    ConversationSummaryBufferMemory, ConversationSummaryMemory,\\\n",
    "    ConversationBufferWindowMemory\n",
    "# Agents and Tools\n",
    "from langchain.agents import  tool # this is a decorator\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentType,initialize_agent,\\\n",
    "    load_tools, Tool,\\\n",
    "        AgentExecutor,\\\n",
    "        create_json_agent,create_react_agent,create_structured_chat_agent\n",
    "from langchain_google_community import GoogleSearchAPIWrapper\n",
    "from langchain.tools import DuckDuckGoSearchRun,DuckDuckGoSearchResults\n",
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain_experimental.autonomous_agents import BabyAGI, AutoGPT\n",
    "from langchain.tools.file_management import WriteFileTool,ReadFileTool\n",
    "from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
    "import faiss\n",
    "# LlamaIndex\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import download_loader\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.core.service_context import ServiceContext\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine,\\\n",
    "    SubQuestionQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.core.indices import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool,ToolMetadata\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.evaluation import generate_question_context_pairs,\\\n",
    "    EmbeddingQAFinetuneDataset,RelevancyEvaluator,\\\n",
    "    FaithfulnessEvaluator,BatchEvalRunner,RetrieverEvaluator\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.instructor import InstructorEmbedding\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank as llama_index_cohere_reranker\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank,LLMRerank\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI as llama_hugging_face_api\n",
    "from llama_index.llms.text_generation_inference import TextGenerationInference\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.readers.github import GithubRepositoryReader,GithubClient\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings as langchain_embeddings_hugging_face_api\n",
    "# from llama_index.llms.langchain import LangChainLLM # causing conflicts\n",
    "# DeepLake\n",
    "import deeplake\n",
    "# Others\n",
    "from newspaper import Article\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "from elevenlabs.client import ElevenLabs\n",
    "from streamlit_chat import message\n",
    "from audio_recorder_streamlit import audio_recorder\n",
    "import streamlit as st\n",
    "import cohere\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import seaborn as sns\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import yt_dlp\n",
    "import whisper\n",
    "import textwrap\n",
    "import re\n",
    "import tiktoken\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Provide the filename as a string\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_new_tokens\": 500, # Maximum tokens to generate\n",
    "    \"max_length\": 2000, # Maximum length of input + output\n",
    "    \"temperature\": 0.1, # Controls randomness of output\n",
    "    \"timeout\": 6000,\n",
    "    # \"task\":'conversational',\n",
    "}\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN,\n",
    "    # you specify the task or not\n",
    "    # You can also specify the task in the model_kwargs or within here\n",
    "    # task = 'conversational',\n",
    "    **model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop = os.path.expanduser(\"~\\Desktop\")\n",
    "llmai = os.path.join(desktop,'llmai')\n",
    "llm = os.path.join(llmai,'llms')\n",
    "projects = os.path.join(llm,'my_llm_projects')\n",
    "information_bot = os.path.join(projects,'information_grabbing')\n",
    "resources = os.path.join(information_bot,\"resources\")\n",
    "books = os.path.join(resources,\"book1.pdf\")\n",
    "print(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(books)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of documents:\\n{len(pages)}\\n\")\n",
    "print(f\"The whole documents:\\n{pages}\\n\")\n",
    "print(f\"Taking a document from the book:\\n{pages[0]}\\n\")\n",
    "print(f\"Taking a page content from the book:\\n{pages[0].page_content}\\n\")\n",
    "print(f\"Taking a meta data from the book:\\n{pages[0].metadata}\\n\")\n",
    "print(f\"Meta data keys: {list(pages[0].metadata.keys())}\\n\")\n",
    "print(f\"Taking a source from meta data from the book:\\n{pages[0].metadata['source']}\\n\")\n",
    "print(f\"Taking a page from meta data from the book:\\n{pages[0].metadata['page']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=500)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of documents:\\n{len(docs)}\\n\")\n",
    "print(f\"The whole documents:\\n{docs}\\n\")\n",
    "print(f\"Taking a document from the book:\\n{docs[0]}\\n\")\n",
    "print(f\"Taking a page content from the book:\\n{docs[0].page_content}\\n\")\n",
    "print(f\"Taking a meta data from the book:\\n{docs[0].metadata}\\n\")\n",
    "print(f\"Meta data keys: {list(docs[0].metadata.keys())}\\n\")\n",
    "print(f\"Taking a source from meta data from the book:\\n{docs[0].metadata['source']}\\n\")\n",
    "print(f\"Taking a page from meta data from the book:\\n{docs[0].metadata['page']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts,all_metadatas = [], []\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    # print(docs[i].page_content)\n",
    "    all_texts.append(docs[i].page_content)\n",
    "    all_metadatas.append({\"source\": f\"Document_{docs[i].metadata['page']}\"})\n",
    "    # print(docs[i].metadata['page'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sowmya Vajjala,  \n",
      "Bodhisattwa Majumder,  \n",
      "Anuj Gupta & Harshit SuranaPractical  \n",
      "Natural Language \n",
      "Processing\n",
      "A Comprehensive Guide to Building  \n",
      "Real-World NLP Systems\n",
      "{'source': 'Document_0'}\n"
     ]
    }
   ],
   "source": [
    "print(all_texts[0])\n",
    "print(all_metadatas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cohere Embeddings\n",
    "# COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "# cohere_embeddings = CohereEmbeddings(\n",
    "#     model=\"embed-english-v2.0\",\n",
    "#     cohere_api_key=COHERE_API_KEY)\n",
    "\n",
    "\n",
    "model=\"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "embeddings = HuggingFaceEndpointEmbeddings(model=model)\n",
    "\n",
    "# Create Deep Lake dataset\n",
    "ACTIVELOOP_TOKEN = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "my_activeloop_org_id = \"danllm\"\n",
    "my_activeloop_dataset_name = \"info_grabber\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding=embeddings,overwrite=True)\n",
    "\n",
    "# add documents to out Deep lake dataset\n",
    "db.add_texts(all_texts,all_metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RetrievalQAWithSourcesChain WITHOUT A RERANKER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_kwargs = {\"k\":4}\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    \"question\": \"what is NLP?\"\n",
    "}\n",
    "d_response = qa.invoke(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is NLP?\n",
      "\n",
      "Source(s):\n"
     ]
    }
   ],
   "source": [
    "print(d_response['question'])\n",
    "print(d_response['answer'])\n",
    "print(\"Source(s):\")\n",
    "for source in d_response['sources'].split():\n",
    "    print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [{\"question\": \"what is NLP?\"}, {\"question\": \"what are the pipelines of NLP?\"}]\n",
    "# d_response = qa.invoke(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is NLP?\n",
      "\n",
      "Source(s):\n",
      "what are the pipelines of NLP?\n",
      "\n",
      "Source(s):\n"
     ]
    }
   ],
   "source": [
    "for question in input_data:\n",
    "    d_response = qa.invoke(question)\n",
    "    print(d_response['question'])\n",
    "    print(d_response['answer'])\n",
    "    print(\"Source(s):\")\n",
    "    for source in d_response['sources'].split():\n",
    "        print(f\"- {source}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **WITH A RERANKER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = langchain_cohere_reranker(model='rerank-english-v3.0')\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\"what are the pipelines of NLP?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We’ll start the discussion by revisiting the NLP pipeline we introduced in Chapter 2\n",
      "and take a look at the last two steps: deployment, followed by monitoring and updat‐\n",
      "ing the model, which we didn’t cover in earlier chapters. We’ll also see what it takes to\n",
      "build and maintain a mature NLP system. This is followed by a discussion on the data\n",
      "science processes followed in various AI teams, especially with respect to building\n",
      "NLP software in particular. We’ll conclude the chapter with a lot of recommenda‐\n",
      "tions, best practices, and do’s and don’ts to successfully deliver NLP projects. Let’s\n",
      "start by looking at how to deploy NLP software.\n",
      "Revisiting the NLP Pipeline: Deploying NLP Software\n",
      "In Chapter 2 , we saw that a typical production pipeline for NLP projects consists of\n",
      "the following stages: data acquisition, text cleaning, text pre-processing, text repre‐\n",
      "sentation and feature engineering, modeling, evaluation, deployment, monitoring,\n",
      "and model updating. When we encounter a new problem scenario involving NLP in\n",
      "our organization, we have to first start thinking about creating an NLP pipeline cov‐\n",
      "ering these stages. Some of the questions we should ask ourselves in this process are:\n",
      "•What kind of data do we need for training the NLP system? Where do we get this\n",
      "data from? These questions are important at the start and also later as the model\n",
      "matures.\n",
      "•How much data is available? If it’s not enough, what data augmentation techni‐\n",
      "ques can we try?\n",
      "•How will we label the data, if necessary?\n",
      "•How will we quantify the performance of our model? What metrics will we use to\n",
      "do that?\n",
      "•How will we deploy the system? Using API calls over the cloud, or a monolith\n",
      "system, or an embedded module on an edge device?\n",
      "•How will the predictions be served: streaming or batch process?\n",
      "•Would we need to update the model? If yes, what will the update frequency be:\n",
      "daily, weekly, monthly?\n",
      "•Do we need a monitoring and alerting mechanism for model performance? If\n"
     ]
    }
   ],
   "source": [
    "print(compressed_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      "problem down into several sub-problems , then try to develop a step-by-step \n",
      "procedure  to solve them. Since language  processing  is involved , we would also\n",
      "list all the forms of text processing  needed at each step. This step-by-step \n",
      "processing  of text is known as pipeline . It is the series of steps involved  in\n",
      "building  any NLP model. These steps are common in every NLP project, so it \n",
      "makes sense to study them in this chapter. Understanding  some common procedures\n",
      "in any NLP pipeline  will enable us to get started on any NLP problem encountered  \n",
      "in the workplace . Laying out and developing  a text-processing  pipeline  is seen \n",
      "as a starting  point for any NLP application  development  process. In this\n",
      "chapter, we will learn about the various steps involved  and how they play  \n",
      "important  roles in solving the NLP problem and we’ll see a few guidelines\n",
      "about when and how to use which step. In later chapters , we’ll discuss  \n",
      "specific  pipelines  for various NLP tasks (e.g., Chapters  4–7).\"\n",
      "my_sentences  = sent_tokenize (mytext)\n",
      "50 | Chapter 2: NLP Pipeline\n",
      "\n",
      "We’ll start the discussion by revisiting the NLP pipeline we introduced in Chapter 2\n",
      "and take a look at the last two steps: deployment, followed by monitoring and updat‐\n",
      "ing the model, which we didn’t cover in earlier chapters. We’ll also see what it takes to\n",
      "build and maintain a mature NLP system. This is followed by a discussion on the data\n",
      "science processes followed in various AI teams, especially with respect to building\n",
      "NLP software in particular. We’ll conclude the chapter with a lot of recommenda‐\n",
      "tions, best practices, and do’s and don’ts to successfully deliver NLP projects. Let’s\n",
      "start by looking at how to deploy NLP software.\n",
      "Revisiting the NLP Pipeline: Deploying NLP Software\n",
      "In Chapter 2 , we saw that a typical production pipeline for NLP projects consists of\n",
      "the following stages: data acquisition, text cleaning, text pre-processing, text repre‐\n",
      "sentation and feature engineering, modeling, evaluation, deployment, monitoring,\n",
      "and model updating. When we encounter a new problem scenario involving NLP in\n",
      "our organization, we have to first start thinking about creating an NLP pipeline cov‐\n",
      "ering these stages. Some of the questions we should ask ourselves in this process are:\n",
      "•What kind of data do we need for training the NLP system? Where do we get this\n",
      "data from? These questions are important at the start and also later as the model\n",
      "matures.\n",
      "•How much data is available? If it’s not enough, what data augmentation techni‐\n",
      "ques can we try?\n",
      "•How will we label the data, if necessary?\n",
      "•How will we quantify the performance of our model? What metrics will we use to\n",
      "do that?\n",
      "•How will we deploy the system? Using API calls over the cloud, or a monolith\n",
      "system, or an embedded module on an edge device?\n",
      "•How will the predictions be served: streaming or batch process?\n",
      "•Would we need to update the model? If yes, what will the update frequency be:\n",
      "daily, weekly, monthly?\n",
      "•Do we need a monitoring and alerting mechanism for model performance? If\n",
      "\n",
      "We also saw how a traditional NLP pipeline and a DL-based NLP pipeline differ from\n",
      "each other and learned what to do when working with non-English languages. Aside\n",
      "from the case study, we looked at these steps in a more general manner in this chap‐\n",
      "ter. Specific details for each step will depend on the task at hand and the purpose of\n",
      "our implementation. We’ll look at a few task-specific pipelines from Chapter 4\n",
      "onward, describing in detail what’s unique as well as common across different tasks\n",
      "while designing such pipelines. In the next chapter, we’ll tackle the question of text\n",
      "representation that we mentioned briefly earlier in this chapter.\n",
      "References\n",
      "[1] Iderhoff, Nicolas. nlp-datasets: Alphabetical list of free/public domain datasets\n",
      "with text data for use in Natural Language Processing (NLP) , (GitHub repo). Last\n",
      "accessed June 15, 2020.\n",
      "[2] Google. “Dataset Search” . Last accessed June 15, 2020.\n",
      "[3] Miller, George A. “WordNet: A Lexical Database for English. ” Communications of\n",
      "the ACM  38.11 (1995): 39–41.\n",
      "[4] NTLTK documentation. “WordNet Interface” . Last accessed June 15, 2020.\n",
      "76 | Chapter 2: NLP Pipeline\n",
      "Source(s):\n",
      "- Document_79\n",
      "- Document_401\n",
      "- Document_105\n"
     ]
    }
   ],
   "source": [
    "retrieved_chunks = [doc.page_content for doc in compressed_docs]\n",
    "\n",
    "# format prompt\n",
    "final_answer = \"\\n\\n\".join(retrieved_chunks).strip()\n",
    "print(f\"Final Answer:\\n{final_answer}\")\n",
    "try:\n",
    "    sources = []\n",
    "    for i in range(len(compressed_docs)):\n",
    "        source = compressed_docs[i].metadata['source']\n",
    "        if not any(source == s for s in sources):\n",
    "            sources.append(source)\n",
    "except IndexError as error:\n",
    "    print(\"No sources\")\n",
    "    sources.append(\"No sources\")\n",
    "\n",
    "print(\"Source(s):\")\n",
    "for source in sources:\n",
    "    print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using the reranker retriever with a QA pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is Natural Language Processing?',\n",
       " 'result': ' Natural Language Processing (NLP) is a field at the intersection of computer'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=compression_retriever\n",
    ")\n",
    "\n",
    "input_data = {\n",
    "    \"query\": \"what is Natural Language Processing?\"\n",
    "}\n",
    "qa.invoke(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RetrievalQAWithSourcesChain wih reranker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = compression_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    # \"question\": \"what is NLP?\",\n",
    "    \"question\": \"what are the pipelines of NLP?\"\n",
    "}\n",
    "d_response = qa.invoke(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what are the pipelines of NLP?', 'answer': '', 'sources': ''}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the pipelines of NLP?\n",
      "\n",
      "Source(s):\n"
     ]
    }
   ],
   "source": [
    "print(d_response['question'])\n",
    "print(d_response['answer'])\n",
    "print(\"Source(s):\")\n",
    "for source in d_response['sources'].split():\n",
    "    print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using Similarity Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      "one-hot  encoding scheme cannot handle this. The only way is to retrain the\n",
      "model: start by expanding the vocabulary, give an ID to the new word, etc.\n",
      "These days, one-hot encoding scheme is seldom used.\n",
      "Some of these shortcomings can be addressed by the bag-of-words approach\n",
      "described next.\n",
      "Bag of Words\n",
      "Bag of words (BoW) is a classical text representation technique that has been used\n",
      "commonly in NLP , especially in text classification problems (see Chapter 4 ). The key\n",
      "idea behind it is as follows: represent the text under consideration as a bag (collec‐\n",
      "tion) of words  while ignoring the order and context. The basic intuition behind it is\n",
      "that it assumes that the text belonging to a given class in the dataset is characterized\n",
      "by a unique set of words. If two text pieces have nearly the same words, then they\n",
      "belong to the same bag (class). Thus, by analyzing the words present in a piece of text,\n",
      "one can identify the class (bag) it belongs to.\n",
      "Similar to one-hot encoding, BoW maps words to unique integer IDs between 1 and\n",
      "|V|. Each document in the corpus is then converted into a vector of |V| dimensions\n",
      "where in the ith component of the vector, i = wid, is simply the number of times the\n",
      "word w occurs in the document, i.e., we simply score each word in V by their occur‐\n",
      "rence count in the document.\n",
      "Thus, for our toy corpus ( Table 3-1 ), where the word IDs are dog = 1, bites = 2, man\n",
      "= 3, meat = 4 , food = 5, eats = 6, D1 becomes [1 1 1 0 0 0]. This is because the first\n",
      "three words in the vocabulary appeared exactly once in D1, and the last three did not\n",
      "appear at all. D4 becomes [0 0 1 0 1 1]. The notebook Ch3/Bag_of_Words.ipynb  dem‐\n",
      "onstrates how we can implement BoW text representation. The following code shows\n",
      "the key parts:\n",
      "from sklearn.feature_extraction.text  import CountVectorizer\n",
      "count_vect  = CountVectorizer ()\n",
      "#Build a BOW representation for the corpus\n",
      "bow_rep = count_vect .fit_transform (processed_docs )\n",
      "#Look at the vocabulary mapping\n",
      "\n",
      "Let’s look at a simple way to implement this in Python from first principles. The note‐\n",
      "book Ch3/OneHotEncoding.ipynb  demonstrates an example of this. The code that fol‐\n",
      "lows is borrowed from the notebook and implements one-hot encoding. In real-\n",
      "world projects, we mostly use scikit-learn’s implementation of one-hot encoding,\n",
      "which is much more optimized. We’ve provided the same in the notebook.\n",
      "Since we assume that the text is tokenized, we can just split the text on white space in\n",
      "this example:\n",
      "def get_onehot_vector (somestring ):\n",
      "  onehot_encoded  = []\n",
      "  for word in somestring .split():\n",
      "             temp = [0]*len(vocab)\n",
      "             if word in vocab:\n",
      "                        temp[vocab[word]-1] = 1\n",
      "             onehot_encoded .append(temp)\n",
      "  return onehot_encoded\n",
      "get_onehot_vector (processed_docs [1])\n",
      "Output: [[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]\n",
      "Now that we understand the scheme, let’s discuss some of its pros and cons. On the\n",
      "positive side, one-hot encoding is intuitive to understand and straightforward to\n",
      "implement. However, it suffers from a few shortcomings:\n",
      "•The size of a one-hot vector is directly proportional to size of the vocabulary, and\n",
      "most real-world corpora have large vocabularies. This results in a sparse repre‐\n",
      "sentation where most of the entries in the vectors are zeroes, making it computa‐\n",
      "tionally inefficient to store, compute with, and learn from (sparsity leads to\n",
      "overfitting).\n",
      "•This representation does not give a fixed-length representation for text, i.e., if a\n",
      "text has 10 words, you get a longer representation for it as compared to a text\n",
      "with 5 words. For most learning algorithms, we need the feature vectors to be of\n",
      "the same length.\n",
      "•It treats words as atomic units and has no notion of (dis)similarity between\n",
      "words. For example, consider three words: run, ran, and apple. Run and ran have\n",
      "similar meanings as opposed to run and apple. But if we take their respective vec‐\n",
      "\n",
      "Building THE Model                                                                                                 65\n",
      "Evaluation                                                                                                                       68\n",
      "Intrinsic Evaluation                                                                                                    68\n",
      "Extrinsic Evaluation                                                                                                   71\n",
      "Post-Modeling Phases                                                                                                   72\n",
      "Deployment                                                                                                                 72\n",
      "Monitoring                                                                                                                  72\n",
      "Model Updating                                                                                                          73\n",
      "Working with Other Languages                                                                                   73\n",
      "Case Study                                                                                                                       74\n",
      "Wrapping Up                                                                                                                  76\n",
      "3.Text Representation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  81\n",
      "Vector Space Models                                                                                                     84\n",
      "Basic Vectorization Approaches                                                                                  85\n",
      "One-Hot Encoding                                                                                                     85\n",
      "Bag of Words                                                                                                               87\n",
      "Source(s):\n",
      "- Document_116\n",
      "- Document_115\n",
      "- Document_9\n"
     ]
    }
   ],
   "source": [
    "# query = \"what are the pipelines of NLP?\"\n",
    "query = \"what is one-hot encoding?\"\n",
    "docs = db.similarity_search(query=query,k=3)\n",
    "retrieved_chunks = [doc.page_content for doc in docs]\n",
    "# format prompt\n",
    "final_answer = \"\\n\\n\".join(retrieved_chunks).strip()\n",
    "print(f\"Final Answer:\\n{final_answer}\")\n",
    "\n",
    "try:\n",
    "    sources = []\n",
    "    for i in range(len(docs)):\n",
    "        source = docs[i].metadata['source']\n",
    "        if not any(source == s for s in sources):\n",
    "            sources.append(source)\n",
    "except IndexError as error:\n",
    "    print(\"No sources\")\n",
    "    sources.append(\"No sources\")\n",
    "\n",
    "print(\"Source(s):\")\n",
    "for source in sources:\n",
    "    print(f\"- {source}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
